{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de objeto pandas datafr-ame\n",
    "patients_df=pd.read_csv('https://github.com/stedy/Machine-Learning-with-R-datasets/blob/master/insurance.csv?raw=true')\n",
    "patients_df.to_csv(\"./data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación variables binarias\n",
    "patients_df.replace({'sex':{'male':0,'female':1}}, inplace=True)\n",
    "patients_df.replace({'smoker':{'yes':1,'no':0}}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region_northeast</th>\n",
       "      <th>region_northwest</th>\n",
       "      <th>region_southeast</th>\n",
       "      <th>region_southwest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1338 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      region_northeast  region_northwest  region_southeast  region_southwest\n",
       "0                    0                 0                 0                 1\n",
       "1                    0                 0                 1                 0\n",
       "2                    0                 0                 1                 0\n",
       "3                    0                 1                 0                 0\n",
       "4                    0                 1                 0                 0\n",
       "...                ...               ...               ...               ...\n",
       "1333                 0                 1                 0                 0\n",
       "1334                 1                 0                 0                 0\n",
       "1335                 0                 0                 1                 0\n",
       "1336                 0                 0                 0                 1\n",
       "1337                 0                 1                 0                 0\n",
       "\n",
       "[1338 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# La función get dummies convierte un DataFrame de columnas categoricas a uno con variables dummy variables\n",
    "region_dummies_df=pd.get_dummies(patients_df[['region']])\n",
    "region_dummies_df = region_dummies_df.replace().astype(int)\n",
    "region_dummies_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos join entre los 2 dataframes para reconstruir el dataset\n",
    "patients_df = patients_df.join(region_dummies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>charges</th>\n",
       "      <th>region_northeast</th>\n",
       "      <th>region_northwest</th>\n",
       "      <th>region_southeast</th>\n",
       "      <th>region_southwest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16884.92400</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1725.55230</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4449.46200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>22.705</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21984.47061</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>28.880</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3866.85520</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>30.970</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>10600.54830</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>31.920</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2205.98080</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>36.850</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1629.83350</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>25.800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2007.94500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>29.070</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>29141.36030</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1338 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  sex     bmi  children  smoker      charges  region_northeast  \\\n",
       "0      19    1  27.900         0       1  16884.92400                 0   \n",
       "1      18    0  33.770         1       0   1725.55230                 0   \n",
       "2      28    0  33.000         3       0   4449.46200                 0   \n",
       "3      33    0  22.705         0       0  21984.47061                 0   \n",
       "4      32    0  28.880         0       0   3866.85520                 0   \n",
       "...   ...  ...     ...       ...     ...          ...               ...   \n",
       "1333   50    0  30.970         3       0  10600.54830                 0   \n",
       "1334   18    1  31.920         0       0   2205.98080                 1   \n",
       "1335   18    1  36.850         0       0   1629.83350                 0   \n",
       "1336   21    1  25.800         0       0   2007.94500                 0   \n",
       "1337   61    1  29.070         0       1  29141.36030                 0   \n",
       "\n",
       "      region_northwest  region_southeast  region_southwest  \n",
       "0                    0                 0                 1  \n",
       "1                    0                 1                 0  \n",
       "2                    0                 1                 0  \n",
       "3                    1                 0                 0  \n",
       "4                    1                 0                 0  \n",
       "...                ...               ...               ...  \n",
       "1333                 1                 0                 0  \n",
       "1334                 0                 0                 0  \n",
       "1335                 0                 1                 0  \n",
       "1336                 0                 0                 1  \n",
       "1337                 1                 0                 0  \n",
       "\n",
       "[1338 rows x 10 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patients_df = patients_df.drop(['region'], axis=1)\n",
    "patients_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso 70% para entrenamiento (random split)\n",
    "train_df= patients_df.sample(frac=0.7,random_state=200)\n",
    "rest_df = patients_df.drop(train_df.index)\n",
    "# Uso 15% para validacion y 15% para test\n",
    "val_df=rest_df.sample(frac=0.5,random_state=200)\n",
    "test_df=rest_df.drop(val_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def polynomial_features(X, degree):\n",
    "    from itertools import combinations_with_replacement\n",
    "    n_samples, n_features = X.shape\n",
    "    combs = [combinations_with_replacement(range(n_features), i) for i in range(0, degree + 1)]\n",
    "    combs = [item for sublist in combs for item in sublist]\n",
    "    n_output_features = len(combs)\n",
    "    X_new = np.empty((n_samples, n_output_features))\n",
    "    for i, index_combs in enumerate(combs):\n",
    "        X_new[:, i] = np.prod(X[:, index_combs], axis=1)\n",
    "    return X_new\n",
    "\n",
    "class linear_regressor():\n",
    "    '''\n",
    "        Clase que implementa el algoritmo de regression logistica con opcion de anadir terminos polinomicos \n",
    "    '''\n",
    "    def __init__(self, degree=1):\n",
    "        \n",
    "        self.theta = None\n",
    "        self.degree = degree\n",
    "        \n",
    "\n",
    "    def fit_model(self, X, Y, lr=0.00001, epochs=100, patience=10, l2=None):\n",
    "        X = polynomial_features(X, self.degree)\n",
    "        self.l2 = l2\n",
    "        n, m = X.shape\n",
    "        self.theta = np.random.rand(m+1,1)\n",
    "        X_c = np.hstack((np.ones((n,1)),X))\n",
    "        loss_v = []\n",
    "        best_loss = np.inf\n",
    "        epochs_stall = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            Y_est = X_c.dot(self.theta)\n",
    "            \n",
    "            # Calcular la funcion de perdida con regularizacion l2 o sin \n",
    "            loss = np.sum(np.power(Y_est-Y,2))/(2.*n) + (self.l2 * np.sum(np.square(self.theta)) / 2*m if self.l2 else 0)\n",
    "            loss_v.append(loss)\n",
    "            \n",
    "            # Calcular gradientes con regularizacion l2 o sin             \n",
    "            regularization_term = (self.l2/m) * self.theta if self.l2 else 0\n",
    "            gradientes = (-1/n) * (X_c.T.dot((Y - Y_est))) + regularization_term\n",
    "            \n",
    "            # Update weights\n",
    "            self.theta = self.theta - lr*gradientes\n",
    "\n",
    "            # Esto se agrega para parar el entrenamiento en caso de que la perdida no disminuya por mas del valor del parametro patience\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_theta = np.copy(self.theta)\n",
    "                epochs_stall = 0\n",
    "            else:\n",
    "                epochs_stall += 1\n",
    "                \n",
    "            if epochs_stall >= patience:\n",
    "                print('La funcion de perdida no ha disminuido, parando despues de {} epocas. el error es: {:.4e}'.format(epoch, loss))\n",
    "                break\n",
    "                \n",
    "            print('Epoch: {} Loss: {:.4e}'.format(epoch, loss))\n",
    "        \n",
    "        print('El error final fue: {:.4e}'.format(loss))\n",
    "        self.theta = best_theta\n",
    "        self.loss_vector = loss_v\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = polynomial_features(X, self.degree)\n",
    "        X_c = np.hstack((np.ones((X.shape[0],1)),X))\n",
    "        Y_hat = X_c.dot(self.theta)\n",
    "        return Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prueba de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 8.5482e+09\n",
      "Epoch: 1 Loss: 3.0887e+20\n",
      "Epoch: 2 Loss: 1.1345e+31\n",
      "Epoch: 3 Loss: 4.1673e+41\n",
      "Epoch: 4 Loss: 1.5307e+52\n",
      "Epoch: 5 Loss: 5.6226e+62\n",
      "Epoch: 6 Loss: 2.0653e+73\n",
      "Epoch: 7 Loss: 7.5862e+83\n",
      "Epoch: 8 Loss: 2.7866e+94\n",
      "Epoch: 9 Loss: 1.0236e+105\n",
      "La funcion de perdida no ha disminuido, parando despues de 10 epocas. el error es: 3.7597e+115\n",
      "El error final fue: 3.7597e+115\n"
     ]
    }
   ],
   "source": [
    "LR = linear_regressor(3)\n",
    "X = train_df.drop(['charges'], axis=1).values\n",
    "Y = train_df[['charges']].values\n",
    "x_test = test_df.drop(['charges'], axis=1).values\n",
    "y_test = test_df['charges'].values\n",
    "LR.fit_model(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El error en test es de 2.998e+09\n"
     ]
    }
   ],
   "source": [
    "y_pred = LR.predict(x_test)\n",
    "\n",
    "# mse\n",
    "mse = np.sum(np.power(y_pred-y_test,2))/(2.*X.shape[0])\n",
    "print(\"El error en test es de {:.4}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisis de variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicolinealidad\n",
    "Como se puede observar no existen variables altamente correlacionadas por lo tanto se decide utilizar todas las variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region_northeast</th>\n",
       "      <th>region_northwest</th>\n",
       "      <th>region_southeast</th>\n",
       "      <th>region_southwest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027422</td>\n",
       "      <td>0.084431</td>\n",
       "      <td>0.060771</td>\n",
       "      <td>-0.051829</td>\n",
       "      <td>-0.010371</td>\n",
       "      <td>0.014073</td>\n",
       "      <td>-0.027887</td>\n",
       "      <td>0.024913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <td>0.027422</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.045460</td>\n",
       "      <td>-0.020528</td>\n",
       "      <td>-0.067844</td>\n",
       "      <td>0.019966</td>\n",
       "      <td>0.030021</td>\n",
       "      <td>-0.019838</td>\n",
       "      <td>-0.028997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bmi</th>\n",
       "      <td>0.084431</td>\n",
       "      <td>-0.045460</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.029462</td>\n",
       "      <td>-0.001460</td>\n",
       "      <td>-0.134960</td>\n",
       "      <td>-0.162882</td>\n",
       "      <td>0.283670</td>\n",
       "      <td>0.003014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>children</th>\n",
       "      <td>0.060771</td>\n",
       "      <td>-0.020528</td>\n",
       "      <td>0.029462</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.000314</td>\n",
       "      <td>-0.053117</td>\n",
       "      <td>0.034247</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.018171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoker</th>\n",
       "      <td>-0.051829</td>\n",
       "      <td>-0.067844</td>\n",
       "      <td>-0.001460</td>\n",
       "      <td>-0.000314</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.045509</td>\n",
       "      <td>-0.049364</td>\n",
       "      <td>0.067197</td>\n",
       "      <td>-0.064973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_northeast</th>\n",
       "      <td>-0.010371</td>\n",
       "      <td>0.019966</td>\n",
       "      <td>-0.134960</td>\n",
       "      <td>-0.053117</td>\n",
       "      <td>0.045509</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.316008</td>\n",
       "      <td>-0.340888</td>\n",
       "      <td>-0.323377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_northwest</th>\n",
       "      <td>0.014073</td>\n",
       "      <td>0.030021</td>\n",
       "      <td>-0.162882</td>\n",
       "      <td>0.034247</td>\n",
       "      <td>-0.049364</td>\n",
       "      <td>-0.316008</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.342887</td>\n",
       "      <td>-0.325274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_southeast</th>\n",
       "      <td>-0.027887</td>\n",
       "      <td>-0.019838</td>\n",
       "      <td>0.283670</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.067197</td>\n",
       "      <td>-0.340888</td>\n",
       "      <td>-0.342887</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.350883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_southwest</th>\n",
       "      <td>0.024913</td>\n",
       "      <td>-0.028997</td>\n",
       "      <td>0.003014</td>\n",
       "      <td>0.018171</td>\n",
       "      <td>-0.064973</td>\n",
       "      <td>-0.323377</td>\n",
       "      <td>-0.325274</td>\n",
       "      <td>-0.350883</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       age       sex       bmi  children    smoker  \\\n",
       "age               1.000000  0.027422  0.084431  0.060771 -0.051829   \n",
       "sex               0.027422  1.000000 -0.045460 -0.020528 -0.067844   \n",
       "bmi               0.084431 -0.045460  1.000000  0.029462 -0.001460   \n",
       "children          0.060771 -0.020528  0.029462  1.000000 -0.000314   \n",
       "smoker           -0.051829 -0.067844 -0.001460 -0.000314  1.000000   \n",
       "region_northeast -0.010371  0.019966 -0.134960 -0.053117  0.045509   \n",
       "region_northwest  0.014073  0.030021 -0.162882  0.034247 -0.049364   \n",
       "region_southeast -0.027887 -0.019838  0.283670  0.000312  0.067197   \n",
       "region_southwest  0.024913 -0.028997  0.003014  0.018171 -0.064973   \n",
       "\n",
       "                  region_northeast  region_northwest  region_southeast  \\\n",
       "age                      -0.010371          0.014073         -0.027887   \n",
       "sex                       0.019966          0.030021         -0.019838   \n",
       "bmi                      -0.134960         -0.162882          0.283670   \n",
       "children                 -0.053117          0.034247          0.000312   \n",
       "smoker                    0.045509         -0.049364          0.067197   \n",
       "region_northeast          1.000000         -0.316008         -0.340888   \n",
       "region_northwest         -0.316008          1.000000         -0.342887   \n",
       "region_southeast         -0.340888         -0.342887          1.000000   \n",
       "region_southwest         -0.323377         -0.325274         -0.350883   \n",
       "\n",
       "                  region_southwest  \n",
       "age                       0.024913  \n",
       "sex                      -0.028997  \n",
       "bmi                       0.003014  \n",
       "children                  0.018171  \n",
       "smoker                   -0.064973  \n",
       "region_northeast         -0.323377  \n",
       "region_northwest         -0.325274  \n",
       "region_southeast         -0.350883  \n",
       "region_southwest          1.000000  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation_matrix = np.corrcoef(X, rowvar=False)\n",
    "\n",
    "# Crear un DataFrame con la matriz de correlación y las etiquetas de las columnas\n",
    "correlation_df = pd.DataFrame(correlation_matrix, columns=patients_df.drop(columns=['charges']).columns, \n",
    "                              index=patients_df.drop(columns=['charges']).columns)\n",
    "\n",
    "correlation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transform(data):\n",
    "    return np.log1p(data)\n",
    "\n",
    "Y_log = log_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 1.7232e+00\n",
      "Epoch: 1 Loss: 1.6810e+00\n",
      "Epoch: 2 Loss: 1.6402e+00\n",
      "Epoch: 3 Loss: 1.6005e+00\n",
      "Epoch: 4 Loss: 1.5619e+00\n",
      "Epoch: 5 Loss: 1.5245e+00\n",
      "Epoch: 6 Loss: 1.4881e+00\n",
      "Epoch: 7 Loss: 1.4528e+00\n",
      "Epoch: 8 Loss: 1.4185e+00\n",
      "Epoch: 9 Loss: 1.3852e+00\n",
      "Epoch: 10 Loss: 1.3528e+00\n",
      "Epoch: 11 Loss: 1.3214e+00\n",
      "Epoch: 12 Loss: 1.2908e+00\n",
      "Epoch: 13 Loss: 1.2611e+00\n",
      "Epoch: 14 Loss: 1.2322e+00\n",
      "Epoch: 15 Loss: 1.2042e+00\n",
      "Epoch: 16 Loss: 1.1769e+00\n",
      "Epoch: 17 Loss: 1.1504e+00\n",
      "Epoch: 18 Loss: 1.1246e+00\n",
      "Epoch: 19 Loss: 1.0996e+00\n",
      "Epoch: 20 Loss: 1.0752e+00\n",
      "Epoch: 21 Loss: 1.0515e+00\n",
      "Epoch: 22 Loss: 1.0284e+00\n",
      "Epoch: 23 Loss: 1.0060e+00\n",
      "Epoch: 24 Loss: 9.8421e-01\n",
      "Epoch: 25 Loss: 9.6300e-01\n",
      "Epoch: 26 Loss: 9.4236e-01\n",
      "Epoch: 27 Loss: 9.2228e-01\n",
      "Epoch: 28 Loss: 9.0275e-01\n",
      "Epoch: 29 Loss: 8.8373e-01\n",
      "Epoch: 30 Loss: 8.6523e-01\n",
      "Epoch: 31 Loss: 8.4722e-01\n",
      "Epoch: 32 Loss: 8.2969e-01\n",
      "Epoch: 33 Loss: 8.1263e-01\n",
      "Epoch: 34 Loss: 7.9602e-01\n",
      "Epoch: 35 Loss: 7.7985e-01\n",
      "Epoch: 36 Loss: 7.6410e-01\n",
      "Epoch: 37 Loss: 7.4877e-01\n",
      "Epoch: 38 Loss: 7.3384e-01\n",
      "Epoch: 39 Loss: 7.1930e-01\n",
      "Epoch: 40 Loss: 7.0513e-01\n",
      "Epoch: 41 Loss: 6.9134e-01\n",
      "Epoch: 42 Loss: 6.7790e-01\n",
      "Epoch: 43 Loss: 6.6481e-01\n",
      "Epoch: 44 Loss: 6.5205e-01\n",
      "Epoch: 45 Loss: 6.3962e-01\n",
      "Epoch: 46 Loss: 6.2752e-01\n",
      "Epoch: 47 Loss: 6.1572e-01\n",
      "Epoch: 48 Loss: 6.0422e-01\n",
      "Epoch: 49 Loss: 5.9301e-01\n",
      "Epoch: 50 Loss: 5.8208e-01\n",
      "Epoch: 51 Loss: 5.7144e-01\n",
      "Epoch: 52 Loss: 5.6106e-01\n",
      "Epoch: 53 Loss: 5.5094e-01\n",
      "Epoch: 54 Loss: 5.4107e-01\n",
      "Epoch: 55 Loss: 5.3145e-01\n",
      "Epoch: 56 Loss: 5.2207e-01\n",
      "Epoch: 57 Loss: 5.1292e-01\n",
      "Epoch: 58 Loss: 5.0400e-01\n",
      "Epoch: 59 Loss: 4.9530e-01\n",
      "Epoch: 60 Loss: 4.8682e-01\n",
      "Epoch: 61 Loss: 4.7854e-01\n",
      "Epoch: 62 Loss: 4.7046e-01\n",
      "Epoch: 63 Loss: 4.6259e-01\n",
      "Epoch: 64 Loss: 4.5490e-01\n",
      "Epoch: 65 Loss: 4.4741e-01\n",
      "Epoch: 66 Loss: 4.4009e-01\n",
      "Epoch: 67 Loss: 4.3295e-01\n",
      "Epoch: 68 Loss: 4.2599e-01\n",
      "Epoch: 69 Loss: 4.1919e-01\n",
      "Epoch: 70 Loss: 4.1255e-01\n",
      "Epoch: 71 Loss: 4.0608e-01\n",
      "Epoch: 72 Loss: 3.9976e-01\n",
      "Epoch: 73 Loss: 3.9359e-01\n",
      "Epoch: 74 Loss: 3.8757e-01\n",
      "Epoch: 75 Loss: 3.8169e-01\n",
      "Epoch: 76 Loss: 3.7595e-01\n",
      "Epoch: 77 Loss: 3.7034e-01\n",
      "Epoch: 78 Loss: 3.6487e-01\n",
      "Epoch: 79 Loss: 3.5953e-01\n",
      "Epoch: 80 Loss: 3.5431e-01\n",
      "Epoch: 81 Loss: 3.4922e-01\n",
      "Epoch: 82 Loss: 3.4424e-01\n",
      "Epoch: 83 Loss: 3.3938e-01\n",
      "Epoch: 84 Loss: 3.3463e-01\n",
      "Epoch: 85 Loss: 3.3000e-01\n",
      "Epoch: 86 Loss: 3.2547e-01\n",
      "Epoch: 87 Loss: 3.2104e-01\n",
      "Epoch: 88 Loss: 3.1672e-01\n",
      "Epoch: 89 Loss: 3.1250e-01\n",
      "Epoch: 90 Loss: 3.0838e-01\n",
      "Epoch: 91 Loss: 3.0435e-01\n",
      "Epoch: 92 Loss: 3.0041e-01\n",
      "Epoch: 93 Loss: 2.9656e-01\n",
      "Epoch: 94 Loss: 2.9280e-01\n",
      "Epoch: 95 Loss: 2.8913e-01\n",
      "Epoch: 96 Loss: 2.8553e-01\n",
      "Epoch: 97 Loss: 2.8203e-01\n",
      "Epoch: 98 Loss: 2.7860e-01\n",
      "Epoch: 99 Loss: 2.7524e-01\n",
      "El error final fue: 2.7524e-01\n"
     ]
    }
   ],
   "source": [
    "# Estandarización de características\n",
    "def standardize(data):\n",
    "    mean = np.mean(data, axis=0)\n",
    "    std = np.std(data, axis=0)\n",
    "    return (data - mean) / std, mean, std\n",
    "\n",
    "\n",
    "X_standardized, mean_X, std_X = standardize(X)\n",
    "Y_standardized, mean_Y, std_Y = standardize(Y)\n",
    "\n",
    "model = linear_regressor()\n",
    "model.fit_model(X_standardized, Y_standardized, lr=0.01, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 1.6038e+08\n",
      "Epoch: 1 Loss: 1.5580e+08\n",
      "Epoch: 2 Loss: 1.5138e+08\n",
      "Epoch: 3 Loss: 1.4712e+08\n",
      "Epoch: 4 Loss: 1.4300e+08\n",
      "Epoch: 5 Loss: 1.3903e+08\n",
      "Epoch: 6 Loss: 1.3519e+08\n",
      "Epoch: 7 Loss: 1.3149e+08\n",
      "Epoch: 8 Loss: 1.2792e+08\n",
      "Epoch: 9 Loss: 1.2447e+08\n",
      "Epoch: 10 Loss: 1.2114e+08\n",
      "Epoch: 11 Loss: 1.1792e+08\n",
      "Epoch: 12 Loss: 1.1482e+08\n",
      "Epoch: 13 Loss: 1.1182e+08\n",
      "Epoch: 14 Loss: 1.0892e+08\n",
      "Epoch: 15 Loss: 1.0612e+08\n",
      "Epoch: 16 Loss: 1.0342e+08\n",
      "Epoch: 17 Loss: 1.0081e+08\n",
      "Epoch: 18 Loss: 9.8283e+07\n",
      "Epoch: 19 Loss: 9.5845e+07\n",
      "Epoch: 20 Loss: 9.3489e+07\n",
      "Epoch: 21 Loss: 9.1212e+07\n",
      "Epoch: 22 Loss: 8.9011e+07\n",
      "Epoch: 23 Loss: 8.6884e+07\n",
      "Epoch: 24 Loss: 8.4827e+07\n",
      "Epoch: 25 Loss: 8.2839e+07\n",
      "Epoch: 26 Loss: 8.0917e+07\n",
      "Epoch: 27 Loss: 7.9058e+07\n",
      "Epoch: 28 Loss: 7.7260e+07\n",
      "Epoch: 29 Loss: 7.5521e+07\n",
      "Epoch: 30 Loss: 7.3839e+07\n",
      "Epoch: 31 Loss: 7.2213e+07\n",
      "Epoch: 32 Loss: 7.0638e+07\n",
      "Epoch: 33 Loss: 6.9116e+07\n",
      "Epoch: 34 Loss: 6.7642e+07\n",
      "Epoch: 35 Loss: 6.6216e+07\n",
      "Epoch: 36 Loss: 6.4835e+07\n",
      "Epoch: 37 Loss: 6.3499e+07\n",
      "Epoch: 38 Loss: 6.2206e+07\n",
      "Epoch: 39 Loss: 6.0954e+07\n",
      "Epoch: 40 Loss: 5.9741e+07\n",
      "Epoch: 41 Loss: 5.8567e+07\n",
      "Epoch: 42 Loss: 5.7430e+07\n",
      "Epoch: 43 Loss: 5.6329e+07\n",
      "Epoch: 44 Loss: 5.5262e+07\n",
      "Epoch: 45 Loss: 5.4228e+07\n",
      "Epoch: 46 Loss: 5.3227e+07\n",
      "Epoch: 47 Loss: 5.2257e+07\n",
      "Epoch: 48 Loss: 5.1316e+07\n",
      "Epoch: 49 Loss: 5.0405e+07\n",
      "Epoch: 50 Loss: 4.9522e+07\n",
      "Epoch: 51 Loss: 4.8665e+07\n",
      "Epoch: 52 Loss: 4.7835e+07\n",
      "Epoch: 53 Loss: 4.7030e+07\n",
      "Epoch: 54 Loss: 4.6250e+07\n",
      "Epoch: 55 Loss: 4.5492e+07\n",
      "Epoch: 56 Loss: 4.4758e+07\n",
      "Epoch: 57 Loss: 4.4046e+07\n",
      "Epoch: 58 Loss: 4.3355e+07\n",
      "Epoch: 59 Loss: 4.2684e+07\n",
      "Epoch: 60 Loss: 4.2034e+07\n",
      "Epoch: 61 Loss: 4.1402e+07\n",
      "Epoch: 62 Loss: 4.0790e+07\n",
      "Epoch: 63 Loss: 4.0195e+07\n",
      "Epoch: 64 Loss: 3.9617e+07\n",
      "Epoch: 65 Loss: 3.9056e+07\n",
      "Epoch: 66 Loss: 3.8512e+07\n",
      "Epoch: 67 Loss: 3.7983e+07\n",
      "Epoch: 68 Loss: 3.7470e+07\n",
      "Epoch: 69 Loss: 3.6971e+07\n",
      "Epoch: 70 Loss: 3.6486e+07\n",
      "Epoch: 71 Loss: 3.6015e+07\n",
      "Epoch: 72 Loss: 3.5558e+07\n",
      "Epoch: 73 Loss: 3.5113e+07\n",
      "Epoch: 74 Loss: 3.4681e+07\n",
      "Epoch: 75 Loss: 3.4261e+07\n",
      "Epoch: 76 Loss: 3.3853e+07\n",
      "Epoch: 77 Loss: 3.3456e+07\n",
      "Epoch: 78 Loss: 3.3070e+07\n",
      "Epoch: 79 Loss: 3.2695e+07\n",
      "Epoch: 80 Loss: 3.2330e+07\n",
      "Epoch: 81 Loss: 3.1975e+07\n",
      "Epoch: 82 Loss: 3.1630e+07\n",
      "Epoch: 83 Loss: 3.1294e+07\n",
      "Epoch: 84 Loss: 3.0967e+07\n",
      "Epoch: 85 Loss: 3.0649e+07\n",
      "Epoch: 86 Loss: 3.0339e+07\n",
      "Epoch: 87 Loss: 3.0038e+07\n",
      "Epoch: 88 Loss: 2.9745e+07\n",
      "Epoch: 89 Loss: 2.9459e+07\n",
      "Epoch: 90 Loss: 2.9181e+07\n",
      "Epoch: 91 Loss: 2.8911e+07\n",
      "Epoch: 92 Loss: 2.8647e+07\n",
      "Epoch: 93 Loss: 2.8391e+07\n",
      "Epoch: 94 Loss: 2.8141e+07\n",
      "Epoch: 95 Loss: 2.7897e+07\n",
      "Epoch: 96 Loss: 2.7660e+07\n",
      "Epoch: 97 Loss: 2.7429e+07\n",
      "Epoch: 98 Loss: 2.7204e+07\n",
      "Epoch: 99 Loss: 2.6985e+07\n",
      "El error final fue: 2.6985e+07\n",
      "Epoch: 0 Loss: 1.6038e+08\n",
      "Epoch: 1 Loss: 1.5582e+08\n",
      "Epoch: 2 Loss: 1.5147e+08\n",
      "Epoch: 3 Loss: 1.4732e+08\n",
      "Epoch: 4 Loss: 1.4335e+08\n",
      "Epoch: 5 Loss: 1.3957e+08\n",
      "Epoch: 6 Loss: 1.3596e+08\n",
      "Epoch: 7 Loss: 1.3252e+08\n",
      "Epoch: 8 Loss: 1.2924e+08\n",
      "Epoch: 9 Loss: 1.2611e+08\n",
      "Epoch: 10 Loss: 1.2313e+08\n",
      "Epoch: 11 Loss: 1.2029e+08\n",
      "Epoch: 12 Loss: 1.1758e+08\n",
      "Epoch: 13 Loss: 1.1501e+08\n",
      "Epoch: 14 Loss: 1.1256e+08\n",
      "Epoch: 15 Loss: 1.1023e+08\n",
      "Epoch: 16 Loss: 1.0801e+08\n",
      "Epoch: 17 Loss: 1.0591e+08\n",
      "Epoch: 18 Loss: 1.0391e+08\n",
      "Epoch: 19 Loss: 1.0201e+08\n",
      "Epoch: 20 Loss: 1.0021e+08\n",
      "Epoch: 21 Loss: 9.8496e+07\n",
      "Epoch: 22 Loss: 9.6874e+07\n",
      "Epoch: 23 Loss: 9.5338e+07\n",
      "Epoch: 24 Loss: 9.3883e+07\n",
      "Epoch: 25 Loss: 9.2506e+07\n",
      "Epoch: 26 Loss: 9.1204e+07\n",
      "Epoch: 27 Loss: 8.9974e+07\n",
      "Epoch: 28 Loss: 8.8812e+07\n",
      "Epoch: 29 Loss: 8.7717e+07\n",
      "Epoch: 30 Loss: 8.6684e+07\n",
      "Epoch: 31 Loss: 8.5712e+07\n",
      "Epoch: 32 Loss: 8.4797e+07\n",
      "Epoch: 33 Loss: 8.3938e+07\n",
      "Epoch: 34 Loss: 8.3131e+07\n",
      "Epoch: 35 Loss: 8.2375e+07\n",
      "Epoch: 36 Loss: 8.1668e+07\n",
      "Epoch: 37 Loss: 8.1008e+07\n",
      "Epoch: 38 Loss: 8.0391e+07\n",
      "Epoch: 39 Loss: 7.9817e+07\n",
      "Epoch: 40 Loss: 7.9284e+07\n",
      "Epoch: 41 Loss: 7.8790e+07\n",
      "Epoch: 42 Loss: 7.8333e+07\n",
      "Epoch: 43 Loss: 7.7911e+07\n",
      "Epoch: 44 Loss: 7.7524e+07\n",
      "Epoch: 45 Loss: 7.7169e+07\n",
      "Epoch: 46 Loss: 7.6845e+07\n",
      "Epoch: 47 Loss: 7.6550e+07\n",
      "Epoch: 48 Loss: 7.6284e+07\n",
      "Epoch: 49 Loss: 7.6045e+07\n",
      "Epoch: 50 Loss: 7.5832e+07\n",
      "Epoch: 51 Loss: 7.5644e+07\n",
      "Epoch: 52 Loss: 7.5479e+07\n",
      "Epoch: 53 Loss: 7.5337e+07\n",
      "Epoch: 54 Loss: 7.5216e+07\n",
      "Epoch: 55 Loss: 7.5116e+07\n",
      "Epoch: 56 Loss: 7.5035e+07\n",
      "Epoch: 57 Loss: 7.4973e+07\n",
      "Epoch: 58 Loss: 7.4928e+07\n",
      "Epoch: 59 Loss: 7.4900e+07\n",
      "Epoch: 60 Loss: 7.4889e+07\n",
      "Epoch: 61 Loss: 7.4893e+07\n",
      "Epoch: 62 Loss: 7.4911e+07\n",
      "Epoch: 63 Loss: 7.4943e+07\n",
      "Epoch: 64 Loss: 7.4989e+07\n",
      "Epoch: 65 Loss: 7.5047e+07\n",
      "Epoch: 66 Loss: 7.5117e+07\n",
      "Epoch: 67 Loss: 7.5198e+07\n",
      "Epoch: 68 Loss: 7.5290e+07\n",
      "Epoch: 69 Loss: 7.5392e+07\n",
      "La funcion de perdida no ha disminuido, parando despues de 70 epocas. el error es: 7.5504e+07\n",
      "El error final fue: 7.5504e+07\n",
      "Epoch: 0 Loss: 1.6038e+08\n",
      "Epoch: 1 Loss: 1.5580e+08\n",
      "Epoch: 2 Loss: 1.5139e+08\n",
      "Epoch: 3 Loss: 1.4714e+08\n",
      "Epoch: 4 Loss: 1.4304e+08\n",
      "Epoch: 5 Loss: 1.3908e+08\n",
      "Epoch: 6 Loss: 1.3527e+08\n",
      "Epoch: 7 Loss: 1.3159e+08\n",
      "Epoch: 8 Loss: 1.2805e+08\n",
      "Epoch: 9 Loss: 1.2463e+08\n",
      "Epoch: 10 Loss: 1.2134e+08\n",
      "Epoch: 11 Loss: 1.1816e+08\n",
      "Epoch: 12 Loss: 1.1509e+08\n",
      "Epoch: 13 Loss: 1.1214e+08\n",
      "Epoch: 14 Loss: 1.0928e+08\n",
      "Epoch: 15 Loss: 1.0653e+08\n",
      "Epoch: 16 Loss: 1.0388e+08\n",
      "Epoch: 17 Loss: 1.0132e+08\n",
      "Epoch: 18 Loss: 9.8846e+07\n",
      "Epoch: 19 Loss: 9.6462e+07\n",
      "Epoch: 20 Loss: 9.4161e+07\n",
      "Epoch: 21 Loss: 9.1941e+07\n",
      "Epoch: 22 Loss: 8.9798e+07\n",
      "Epoch: 23 Loss: 8.7730e+07\n",
      "Epoch: 24 Loss: 8.5734e+07\n",
      "Epoch: 25 Loss: 8.3807e+07\n",
      "Epoch: 26 Loss: 8.1947e+07\n",
      "Epoch: 27 Loss: 8.0151e+07\n",
      "Epoch: 28 Loss: 7.8417e+07\n",
      "Epoch: 29 Loss: 7.6743e+07\n",
      "Epoch: 30 Loss: 7.5126e+07\n",
      "Epoch: 31 Loss: 7.3565e+07\n",
      "Epoch: 32 Loss: 7.2057e+07\n",
      "Epoch: 33 Loss: 7.0601e+07\n",
      "Epoch: 34 Loss: 6.9194e+07\n",
      "Epoch: 35 Loss: 6.7835e+07\n",
      "Epoch: 36 Loss: 6.6523e+07\n",
      "Epoch: 37 Loss: 6.5254e+07\n",
      "Epoch: 38 Loss: 6.4029e+07\n",
      "Epoch: 39 Loss: 6.2845e+07\n",
      "Epoch: 40 Loss: 6.1701e+07\n",
      "Epoch: 41 Loss: 6.0595e+07\n",
      "Epoch: 42 Loss: 5.9526e+07\n",
      "Epoch: 43 Loss: 5.8493e+07\n",
      "Epoch: 44 Loss: 5.7495e+07\n",
      "Epoch: 45 Loss: 5.6529e+07\n",
      "Epoch: 46 Loss: 5.5596e+07\n",
      "Epoch: 47 Loss: 5.4694e+07\n",
      "Epoch: 48 Loss: 5.3822e+07\n",
      "Epoch: 49 Loss: 5.2978e+07\n",
      "Epoch: 50 Loss: 5.2162e+07\n",
      "Epoch: 51 Loss: 5.1373e+07\n",
      "Epoch: 52 Loss: 5.0610e+07\n",
      "Epoch: 53 Loss: 4.9872e+07\n",
      "Epoch: 54 Loss: 4.9157e+07\n",
      "Epoch: 55 Loss: 4.8466e+07\n",
      "Epoch: 56 Loss: 4.7798e+07\n",
      "Epoch: 57 Loss: 4.7151e+07\n",
      "Epoch: 58 Loss: 4.6525e+07\n",
      "Epoch: 59 Loss: 4.5920e+07\n",
      "Epoch: 60 Loss: 4.5333e+07\n",
      "Epoch: 61 Loss: 4.4766e+07\n",
      "Epoch: 62 Loss: 4.4217e+07\n",
      "Epoch: 63 Loss: 4.3685e+07\n",
      "Epoch: 64 Loss: 4.3170e+07\n",
      "Epoch: 65 Loss: 4.2672e+07\n",
      "Epoch: 66 Loss: 4.2190e+07\n",
      "Epoch: 67 Loss: 4.1722e+07\n",
      "Epoch: 68 Loss: 4.1270e+07\n",
      "Epoch: 69 Loss: 4.0832e+07\n",
      "Epoch: 70 Loss: 4.0407e+07\n",
      "Epoch: 71 Loss: 3.9996e+07\n",
      "Epoch: 72 Loss: 3.9598e+07\n",
      "Epoch: 73 Loss: 3.9212e+07\n",
      "Epoch: 74 Loss: 3.8839e+07\n",
      "Epoch: 75 Loss: 3.8476e+07\n",
      "Epoch: 76 Loss: 3.8126e+07\n",
      "Epoch: 77 Loss: 3.7786e+07\n",
      "Epoch: 78 Loss: 3.7456e+07\n",
      "Epoch: 79 Loss: 3.7137e+07\n",
      "Epoch: 80 Loss: 3.6827e+07\n",
      "Epoch: 81 Loss: 3.6527e+07\n",
      "Epoch: 82 Loss: 3.6236e+07\n",
      "Epoch: 83 Loss: 3.5954e+07\n",
      "Epoch: 84 Loss: 3.5681e+07\n",
      "Epoch: 85 Loss: 3.5416e+07\n",
      "Epoch: 86 Loss: 3.5159e+07\n",
      "Epoch: 87 Loss: 3.4910e+07\n",
      "Epoch: 88 Loss: 3.4668e+07\n",
      "Epoch: 89 Loss: 3.4434e+07\n",
      "Epoch: 90 Loss: 3.4207e+07\n",
      "Epoch: 91 Loss: 3.3986e+07\n",
      "Epoch: 92 Loss: 3.3773e+07\n",
      "Epoch: 93 Loss: 3.3565e+07\n",
      "Epoch: 94 Loss: 3.3364e+07\n",
      "Epoch: 95 Loss: 3.3169e+07\n",
      "Epoch: 96 Loss: 3.2979e+07\n",
      "Epoch: 97 Loss: 3.2796e+07\n",
      "Epoch: 98 Loss: 3.2617e+07\n",
      "Epoch: 99 Loss: 3.2444e+07\n",
      "El error final fue: 3.2444e+07\n",
      "Epoch: 0 Loss: 1.6033e+08\n",
      "Epoch: 1 Loss: 1.3423e+08\n",
      "Epoch: 2 Loss: 1.1388e+08\n",
      "Epoch: 3 Loss: 9.7932e+07\n",
      "Epoch: 4 Loss: 8.5377e+07\n",
      "Epoch: 5 Loss: 7.5433e+07\n",
      "Epoch: 6 Loss: 6.7502e+07\n",
      "Epoch: 7 Loss: 6.1129e+07\n",
      "Epoch: 8 Loss: 5.5961e+07\n",
      "Epoch: 9 Loss: 5.1731e+07\n",
      "Epoch: 10 Loss: 4.8232e+07\n",
      "Epoch: 11 Loss: 4.5305e+07\n",
      "Epoch: 12 Loss: 4.2829e+07\n",
      "Epoch: 13 Loss: 4.0709e+07\n",
      "Epoch: 14 Loss: 3.8873e+07\n",
      "Epoch: 15 Loss: 3.7264e+07\n",
      "Epoch: 16 Loss: 3.5840e+07\n",
      "Epoch: 17 Loss: 3.4567e+07\n",
      "Epoch: 18 Loss: 3.3417e+07\n",
      "Epoch: 19 Loss: 3.2371e+07\n",
      "Epoch: 20 Loss: 3.1412e+07\n",
      "Epoch: 21 Loss: 3.0527e+07\n",
      "Epoch: 22 Loss: 2.9706e+07\n",
      "Epoch: 23 Loss: 2.8941e+07\n",
      "Epoch: 24 Loss: 2.8225e+07\n",
      "Epoch: 25 Loss: 2.7552e+07\n",
      "Epoch: 26 Loss: 2.6918e+07\n",
      "Epoch: 27 Loss: 2.6320e+07\n",
      "Epoch: 28 Loss: 2.5753e+07\n",
      "Epoch: 29 Loss: 2.5216e+07\n",
      "Epoch: 30 Loss: 2.4706e+07\n",
      "Epoch: 31 Loss: 2.4220e+07\n",
      "Epoch: 32 Loss: 2.3758e+07\n",
      "Epoch: 33 Loss: 2.3318e+07\n",
      "Epoch: 34 Loss: 2.2897e+07\n",
      "Epoch: 35 Loss: 2.2496e+07\n",
      "Epoch: 36 Loss: 2.2112e+07\n",
      "Epoch: 37 Loss: 2.1745e+07\n",
      "Epoch: 38 Loss: 2.1394e+07\n",
      "Epoch: 39 Loss: 2.1058e+07\n",
      "Epoch: 40 Loss: 2.0736e+07\n",
      "Epoch: 41 Loss: 2.0427e+07\n",
      "Epoch: 42 Loss: 2.0131e+07\n",
      "Epoch: 43 Loss: 1.9846e+07\n",
      "Epoch: 44 Loss: 1.9573e+07\n",
      "Epoch: 45 Loss: 1.9311e+07\n",
      "Epoch: 46 Loss: 1.9059e+07\n",
      "Epoch: 47 Loss: 1.8816e+07\n",
      "Epoch: 48 Loss: 1.8583e+07\n",
      "Epoch: 49 Loss: 1.8358e+07\n",
      "Epoch: 50 Loss: 1.8141e+07\n",
      "Epoch: 51 Loss: 1.7933e+07\n",
      "Epoch: 52 Loss: 1.7732e+07\n",
      "Epoch: 53 Loss: 1.7538e+07\n",
      "Epoch: 54 Loss: 1.7350e+07\n",
      "Epoch: 55 Loss: 1.7170e+07\n",
      "Epoch: 56 Loss: 1.6995e+07\n",
      "Epoch: 57 Loss: 1.6827e+07\n",
      "Epoch: 58 Loss: 1.6664e+07\n",
      "Epoch: 59 Loss: 1.6506e+07\n",
      "Epoch: 60 Loss: 1.6354e+07\n",
      "Epoch: 61 Loss: 1.6206e+07\n",
      "Epoch: 62 Loss: 1.6063e+07\n",
      "Epoch: 63 Loss: 1.5925e+07\n",
      "Epoch: 64 Loss: 1.5791e+07\n",
      "Epoch: 65 Loss: 1.5661e+07\n",
      "Epoch: 66 Loss: 1.5535e+07\n",
      "Epoch: 67 Loss: 1.5413e+07\n",
      "Epoch: 68 Loss: 1.5294e+07\n",
      "Epoch: 69 Loss: 1.5179e+07\n",
      "Epoch: 70 Loss: 1.5067e+07\n",
      "Epoch: 71 Loss: 1.4958e+07\n",
      "Epoch: 72 Loss: 1.4853e+07\n",
      "Epoch: 73 Loss: 1.4750e+07\n",
      "Epoch: 74 Loss: 1.4651e+07\n",
      "Epoch: 75 Loss: 1.4554e+07\n",
      "Epoch: 76 Loss: 1.4459e+07\n",
      "Epoch: 77 Loss: 1.4367e+07\n",
      "Epoch: 78 Loss: 1.4278e+07\n",
      "Epoch: 79 Loss: 1.4191e+07\n",
      "Epoch: 80 Loss: 1.4106e+07\n",
      "Epoch: 81 Loss: 1.4023e+07\n",
      "Epoch: 82 Loss: 1.3943e+07\n",
      "Epoch: 83 Loss: 1.3864e+07\n",
      "Epoch: 84 Loss: 1.3788e+07\n",
      "Epoch: 85 Loss: 1.3713e+07\n",
      "Epoch: 86 Loss: 1.3640e+07\n",
      "Epoch: 87 Loss: 1.3569e+07\n",
      "Epoch: 88 Loss: 1.3500e+07\n",
      "Epoch: 89 Loss: 1.3432e+07\n",
      "Epoch: 90 Loss: 1.3366e+07\n",
      "Epoch: 91 Loss: 1.3301e+07\n",
      "Epoch: 92 Loss: 1.3238e+07\n",
      "Epoch: 93 Loss: 1.3177e+07\n",
      "Epoch: 94 Loss: 1.3117e+07\n",
      "Epoch: 95 Loss: 1.3058e+07\n",
      "Epoch: 96 Loss: 1.3000e+07\n",
      "Epoch: 97 Loss: 1.2944e+07\n",
      "Epoch: 98 Loss: 1.2889e+07\n",
      "Epoch: 99 Loss: 1.2835e+07\n",
      "El error final fue: 1.2835e+07\n",
      "Epoch: 0 Loss: 1.6033e+08\n",
      "Epoch: 1 Loss: 1.3500e+08\n",
      "Epoch: 2 Loss: 1.1659e+08\n",
      "Epoch: 3 Loss: 1.0335e+08\n",
      "Epoch: 4 Loss: 9.3973e+07\n",
      "Epoch: 5 Loss: 8.7455e+07\n",
      "Epoch: 6 Loss: 8.3056e+07\n",
      "Epoch: 7 Loss: 8.0216e+07\n",
      "Epoch: 8 Loss: 7.8516e+07\n",
      "Epoch: 9 Loss: 7.7645e+07\n",
      "Epoch: 10 Loss: 7.7370e+07\n",
      "Epoch: 11 Loss: 7.7519e+07\n",
      "Epoch: 12 Loss: 7.7968e+07\n",
      "Epoch: 13 Loss: 7.8622e+07\n",
      "Epoch: 14 Loss: 7.9417e+07\n",
      "Epoch: 15 Loss: 8.0302e+07\n",
      "Epoch: 16 Loss: 8.1244e+07\n",
      "Epoch: 17 Loss: 8.2220e+07\n",
      "Epoch: 18 Loss: 8.3212e+07\n",
      "Epoch: 19 Loss: 8.4209e+07\n",
      "La funcion de perdida no ha disminuido, parando despues de 20 epocas. el error es: 8.5205e+07\n",
      "El error final fue: 8.5205e+07\n",
      "Epoch: 0 Loss: 1.6035e+08\n",
      "Epoch: 1 Loss: 1.3433e+08\n",
      "Epoch: 2 Loss: 1.1417e+08\n",
      "Epoch: 3 Loss: 9.8488e+07\n",
      "Epoch: 4 Loss: 8.6249e+07\n",
      "Epoch: 5 Loss: 7.6645e+07\n",
      "Epoch: 6 Loss: 6.9067e+07\n",
      "Epoch: 7 Loss: 6.3046e+07\n",
      "Epoch: 8 Loss: 5.8225e+07\n",
      "Epoch: 9 Loss: 5.4330e+07\n",
      "Epoch: 10 Loss: 5.1152e+07\n",
      "Epoch: 11 Loss: 4.8533e+07\n",
      "Epoch: 12 Loss: 4.6349e+07\n",
      "Epoch: 13 Loss: 4.4506e+07\n",
      "Epoch: 14 Loss: 4.2932e+07\n",
      "Epoch: 15 Loss: 4.1573e+07\n",
      "Epoch: 16 Loss: 4.0386e+07\n",
      "Epoch: 17 Loss: 3.9337e+07\n",
      "Epoch: 18 Loss: 3.8401e+07\n",
      "Epoch: 19 Loss: 3.7559e+07\n",
      "Epoch: 20 Loss: 3.6796e+07\n",
      "Epoch: 21 Loss: 3.6098e+07\n",
      "Epoch: 22 Loss: 3.5458e+07\n",
      "Epoch: 23 Loss: 3.4866e+07\n",
      "Epoch: 24 Loss: 3.4317e+07\n",
      "Epoch: 25 Loss: 3.3807e+07\n",
      "Epoch: 26 Loss: 3.3331e+07\n",
      "Epoch: 27 Loss: 3.2885e+07\n",
      "Epoch: 28 Loss: 3.2468e+07\n",
      "Epoch: 29 Loss: 3.2077e+07\n",
      "Epoch: 30 Loss: 3.1709e+07\n",
      "Epoch: 31 Loss: 3.1363e+07\n",
      "Epoch: 32 Loss: 3.1037e+07\n",
      "Epoch: 33 Loss: 3.0731e+07\n",
      "Epoch: 34 Loss: 3.0442e+07\n",
      "Epoch: 35 Loss: 3.0170e+07\n",
      "Epoch: 36 Loss: 2.9914e+07\n",
      "Epoch: 37 Loss: 2.9673e+07\n",
      "Epoch: 38 Loss: 2.9445e+07\n",
      "Epoch: 39 Loss: 2.9231e+07\n",
      "Epoch: 40 Loss: 2.9028e+07\n",
      "Epoch: 41 Loss: 2.8838e+07\n",
      "Epoch: 42 Loss: 2.8658e+07\n",
      "Epoch: 43 Loss: 2.8489e+07\n",
      "Epoch: 44 Loss: 2.8329e+07\n",
      "Epoch: 45 Loss: 2.8179e+07\n",
      "Epoch: 46 Loss: 2.8038e+07\n",
      "Epoch: 47 Loss: 2.7904e+07\n",
      "Epoch: 48 Loss: 2.7779e+07\n",
      "Epoch: 49 Loss: 2.7661e+07\n",
      "Epoch: 50 Loss: 2.7550e+07\n",
      "Epoch: 51 Loss: 2.7445e+07\n",
      "Epoch: 52 Loss: 2.7347e+07\n",
      "Epoch: 53 Loss: 2.7254e+07\n",
      "Epoch: 54 Loss: 2.7168e+07\n",
      "Epoch: 55 Loss: 2.7086e+07\n",
      "Epoch: 56 Loss: 2.7010e+07\n",
      "Epoch: 57 Loss: 2.6938e+07\n",
      "Epoch: 58 Loss: 2.6871e+07\n",
      "Epoch: 59 Loss: 2.6807e+07\n",
      "Epoch: 60 Loss: 2.6748e+07\n",
      "Epoch: 61 Loss: 2.6693e+07\n",
      "Epoch: 62 Loss: 2.6641e+07\n",
      "Epoch: 63 Loss: 2.6593e+07\n",
      "Epoch: 64 Loss: 2.6548e+07\n",
      "Epoch: 65 Loss: 2.6506e+07\n",
      "Epoch: 66 Loss: 2.6467e+07\n",
      "Epoch: 67 Loss: 2.6431e+07\n",
      "Epoch: 68 Loss: 2.6397e+07\n",
      "Epoch: 69 Loss: 2.6366e+07\n",
      "Epoch: 70 Loss: 2.6337e+07\n",
      "Epoch: 71 Loss: 2.6311e+07\n",
      "Epoch: 72 Loss: 2.6286e+07\n",
      "Epoch: 73 Loss: 2.6264e+07\n",
      "Epoch: 74 Loss: 2.6243e+07\n",
      "Epoch: 75 Loss: 2.6224e+07\n",
      "Epoch: 76 Loss: 2.6207e+07\n",
      "Epoch: 77 Loss: 2.6192e+07\n",
      "Epoch: 78 Loss: 2.6178e+07\n",
      "Epoch: 79 Loss: 2.6166e+07\n",
      "Epoch: 80 Loss: 2.6155e+07\n",
      "Epoch: 81 Loss: 2.6146e+07\n",
      "Epoch: 82 Loss: 2.6138e+07\n",
      "Epoch: 83 Loss: 2.6131e+07\n",
      "Epoch: 84 Loss: 2.6125e+07\n",
      "Epoch: 85 Loss: 2.6120e+07\n",
      "Epoch: 86 Loss: 2.6117e+07\n",
      "Epoch: 87 Loss: 2.6114e+07\n",
      "Epoch: 88 Loss: 2.6112e+07\n",
      "Epoch: 89 Loss: 2.6112e+07\n",
      "Epoch: 90 Loss: 2.6112e+07\n",
      "Epoch: 91 Loss: 2.6113e+07\n",
      "Epoch: 92 Loss: 2.6115e+07\n",
      "Epoch: 93 Loss: 2.6117e+07\n",
      "Epoch: 94 Loss: 2.6121e+07\n",
      "Epoch: 95 Loss: 2.6125e+07\n",
      "Epoch: 96 Loss: 2.6129e+07\n",
      "Epoch: 97 Loss: 2.6135e+07\n",
      "Epoch: 98 Loss: 2.6141e+07\n",
      "La funcion de perdida no ha disminuido, parando despues de 99 epocas. el error es: 2.6147e+07\n",
      "El error final fue: 2.6147e+07\n",
      "Epoch: 0 Loss: 1.6022e+08\n",
      "Epoch: 1 Loss: 9.3159e+07\n",
      "Epoch: 2 Loss: 5.8810e+07\n",
      "Epoch: 3 Loss: 4.0632e+07\n",
      "Epoch: 4 Loss: 3.0640e+07\n",
      "Epoch: 5 Loss: 2.4901e+07\n",
      "Epoch: 6 Loss: 2.1438e+07\n",
      "Epoch: 7 Loss: 1.9233e+07\n",
      "Epoch: 8 Loss: 1.7748e+07\n",
      "Epoch: 9 Loss: 1.6695e+07\n",
      "Epoch: 10 Loss: 1.5908e+07\n",
      "Epoch: 11 Loss: 1.5296e+07\n",
      "Epoch: 12 Loss: 1.4802e+07\n",
      "Epoch: 13 Loss: 1.4391e+07\n",
      "Epoch: 14 Loss: 1.4041e+07\n",
      "Epoch: 15 Loss: 1.3738e+07\n",
      "Epoch: 16 Loss: 1.3471e+07\n",
      "Epoch: 17 Loss: 1.3233e+07\n",
      "Epoch: 18 Loss: 1.3019e+07\n",
      "Epoch: 19 Loss: 1.2825e+07\n",
      "Epoch: 20 Loss: 1.2647e+07\n",
      "Epoch: 21 Loss: 1.2485e+07\n",
      "Epoch: 22 Loss: 1.2334e+07\n",
      "Epoch: 23 Loss: 1.2195e+07\n",
      "Epoch: 24 Loss: 1.2066e+07\n",
      "Epoch: 25 Loss: 1.1946e+07\n",
      "Epoch: 26 Loss: 1.1834e+07\n",
      "Epoch: 27 Loss: 1.1729e+07\n",
      "Epoch: 28 Loss: 1.1630e+07\n",
      "Epoch: 29 Loss: 1.1537e+07\n",
      "Epoch: 30 Loss: 1.1450e+07\n",
      "Epoch: 31 Loss: 1.1368e+07\n",
      "Epoch: 32 Loss: 1.1291e+07\n",
      "Epoch: 33 Loss: 1.1218e+07\n",
      "Epoch: 34 Loss: 1.1149e+07\n",
      "Epoch: 35 Loss: 1.1084e+07\n",
      "Epoch: 36 Loss: 1.1022e+07\n",
      "Epoch: 37 Loss: 1.0963e+07\n",
      "Epoch: 38 Loss: 1.0907e+07\n",
      "Epoch: 39 Loss: 1.0854e+07\n",
      "Epoch: 40 Loss: 1.0804e+07\n",
      "Epoch: 41 Loss: 1.0756e+07\n",
      "Epoch: 42 Loss: 1.0711e+07\n",
      "Epoch: 43 Loss: 1.0667e+07\n",
      "Epoch: 44 Loss: 1.0626e+07\n",
      "Epoch: 45 Loss: 1.0586e+07\n",
      "Epoch: 46 Loss: 1.0549e+07\n",
      "Epoch: 47 Loss: 1.0513e+07\n",
      "Epoch: 48 Loss: 1.0478e+07\n",
      "Epoch: 49 Loss: 1.0445e+07\n",
      "Epoch: 50 Loss: 1.0414e+07\n",
      "Epoch: 51 Loss: 1.0383e+07\n",
      "Epoch: 52 Loss: 1.0354e+07\n",
      "Epoch: 53 Loss: 1.0327e+07\n",
      "Epoch: 54 Loss: 1.0300e+07\n",
      "Epoch: 55 Loss: 1.0274e+07\n",
      "Epoch: 56 Loss: 1.0250e+07\n",
      "Epoch: 57 Loss: 1.0226e+07\n",
      "Epoch: 58 Loss: 1.0204e+07\n",
      "Epoch: 59 Loss: 1.0182e+07\n",
      "Epoch: 60 Loss: 1.0161e+07\n",
      "Epoch: 61 Loss: 1.0140e+07\n",
      "Epoch: 62 Loss: 1.0121e+07\n",
      "Epoch: 63 Loss: 1.0102e+07\n",
      "Epoch: 64 Loss: 1.0084e+07\n",
      "Epoch: 65 Loss: 1.0066e+07\n",
      "Epoch: 66 Loss: 1.0049e+07\n",
      "Epoch: 67 Loss: 1.0033e+07\n",
      "Epoch: 68 Loss: 1.0017e+07\n",
      "Epoch: 69 Loss: 1.0002e+07\n",
      "Epoch: 70 Loss: 9.9871e+06\n",
      "Epoch: 71 Loss: 9.9728e+06\n",
      "Epoch: 72 Loss: 9.9589e+06\n",
      "Epoch: 73 Loss: 9.9455e+06\n",
      "Epoch: 74 Loss: 9.9325e+06\n",
      "Epoch: 75 Loss: 9.9199e+06\n",
      "Epoch: 76 Loss: 9.9076e+06\n",
      "Epoch: 77 Loss: 9.8958e+06\n",
      "Epoch: 78 Loss: 9.8842e+06\n",
      "Epoch: 79 Loss: 9.8731e+06\n",
      "Epoch: 80 Loss: 9.8622e+06\n",
      "Epoch: 81 Loss: 9.8516e+06\n",
      "Epoch: 82 Loss: 9.8414e+06\n",
      "Epoch: 83 Loss: 9.8314e+06\n",
      "Epoch: 84 Loss: 9.8217e+06\n",
      "Epoch: 85 Loss: 9.8122e+06\n",
      "Epoch: 86 Loss: 9.8031e+06\n",
      "Epoch: 87 Loss: 9.7941e+06\n",
      "Epoch: 88 Loss: 9.7854e+06\n",
      "Epoch: 89 Loss: 9.7769e+06\n",
      "Epoch: 90 Loss: 9.7686e+06\n",
      "Epoch: 91 Loss: 9.7605e+06\n",
      "Epoch: 92 Loss: 9.7527e+06\n",
      "Epoch: 93 Loss: 9.7450e+06\n",
      "Epoch: 94 Loss: 9.7375e+06\n",
      "Epoch: 95 Loss: 9.7302e+06\n",
      "Epoch: 96 Loss: 9.7230e+06\n",
      "Epoch: 97 Loss: 9.7161e+06\n",
      "Epoch: 98 Loss: 9.7093e+06\n",
      "Epoch: 99 Loss: 9.7026e+06\n",
      "El error final fue: 9.7026e+06\n",
      "Epoch: 0 Loss: 1.6023e+08\n",
      "Epoch: 1 Loss: 1.0183e+08\n",
      "Epoch: 2 Loss: 8.4153e+07\n",
      "Epoch: 3 Loss: 8.3414e+07\n",
      "Epoch: 4 Loss: 8.9026e+07\n",
      "Epoch: 5 Loss: 9.6408e+07\n",
      "Epoch: 6 Loss: 1.0369e+08\n",
      "Epoch: 7 Loss: 1.1020e+08\n",
      "Epoch: 8 Loss: 1.1581e+08\n",
      "Epoch: 9 Loss: 1.2055e+08\n",
      "Epoch: 10 Loss: 1.2457e+08\n",
      "Epoch: 11 Loss: 1.2799e+08\n",
      "Epoch: 12 Loss: 1.3093e+08\n",
      "La funcion de perdida no ha disminuido, parando despues de 13 epocas. el error es: 1.3349e+08\n",
      "El error final fue: 1.3349e+08\n",
      "Epoch: 0 Loss: 1.6022e+08\n",
      "Epoch: 1 Loss: 9.4027e+07\n",
      "Epoch: 2 Loss: 6.1346e+07\n",
      "Epoch: 3 Loss: 4.4912e+07\n",
      "Epoch: 4 Loss: 3.6480e+07\n",
      "Epoch: 5 Loss: 3.2053e+07\n",
      "Epoch: 6 Loss: 2.9664e+07\n",
      "Epoch: 7 Loss: 2.8331e+07\n",
      "Epoch: 8 Loss: 2.7555e+07\n",
      "Epoch: 9 Loss: 2.7081e+07\n",
      "Epoch: 10 Loss: 2.6775e+07\n",
      "Epoch: 11 Loss: 2.6566e+07\n",
      "Epoch: 12 Loss: 2.6415e+07\n",
      "Epoch: 13 Loss: 2.6301e+07\n",
      "Epoch: 14 Loss: 2.6212e+07\n",
      "Epoch: 15 Loss: 2.6140e+07\n",
      "Epoch: 16 Loss: 2.6083e+07\n",
      "Epoch: 17 Loss: 2.6036e+07\n",
      "Epoch: 18 Loss: 2.5998e+07\n",
      "Epoch: 19 Loss: 2.5967e+07\n",
      "Epoch: 20 Loss: 2.5944e+07\n",
      "Epoch: 21 Loss: 2.5927e+07\n",
      "Epoch: 22 Loss: 2.5914e+07\n",
      "Epoch: 23 Loss: 2.5907e+07\n",
      "Epoch: 24 Loss: 2.5905e+07\n",
      "Epoch: 25 Loss: 2.5906e+07\n",
      "Epoch: 26 Loss: 2.5911e+07\n",
      "Epoch: 27 Loss: 2.5920e+07\n",
      "Epoch: 28 Loss: 2.5932e+07\n",
      "Epoch: 29 Loss: 2.5946e+07\n",
      "Epoch: 30 Loss: 2.5963e+07\n",
      "Epoch: 31 Loss: 2.5983e+07\n",
      "Epoch: 32 Loss: 2.6005e+07\n",
      "Epoch: 33 Loss: 2.6028e+07\n",
      "La funcion de perdida no ha disminuido, parando despues de 34 epocas. el error es: 2.6054e+07\n",
      "El error final fue: 2.6054e+07\n",
      "Epoch: 0 Loss: 1.6038e+08\n",
      "Epoch: 1 Loss: 1.5992e+08\n",
      "Epoch: 2 Loss: 1.5946e+08\n",
      "Epoch: 3 Loss: 1.5900e+08\n",
      "Epoch: 4 Loss: 1.5854e+08\n",
      "Epoch: 5 Loss: 1.5809e+08\n",
      "Epoch: 6 Loss: 1.5763e+08\n",
      "Epoch: 7 Loss: 1.5718e+08\n",
      "Epoch: 8 Loss: 1.5673e+08\n",
      "Epoch: 9 Loss: 1.5628e+08\n",
      "Epoch: 10 Loss: 1.5584e+08\n",
      "Epoch: 11 Loss: 1.5539e+08\n",
      "Epoch: 12 Loss: 1.5495e+08\n",
      "Epoch: 13 Loss: 1.5450e+08\n",
      "Epoch: 14 Loss: 1.5406e+08\n",
      "Epoch: 15 Loss: 1.5362e+08\n",
      "Epoch: 16 Loss: 1.5318e+08\n",
      "Epoch: 17 Loss: 1.5275e+08\n",
      "Epoch: 18 Loss: 1.5231e+08\n",
      "Epoch: 19 Loss: 1.5188e+08\n",
      "Epoch: 20 Loss: 1.5145e+08\n",
      "Epoch: 21 Loss: 1.5102e+08\n",
      "Epoch: 22 Loss: 1.5059e+08\n",
      "Epoch: 23 Loss: 1.5016e+08\n",
      "Epoch: 24 Loss: 1.4974e+08\n",
      "Epoch: 25 Loss: 1.4931e+08\n",
      "Epoch: 26 Loss: 1.4889e+08\n",
      "Epoch: 27 Loss: 1.4847e+08\n",
      "Epoch: 28 Loss: 1.4805e+08\n",
      "Epoch: 29 Loss: 1.4763e+08\n",
      "Epoch: 30 Loss: 1.4721e+08\n",
      "Epoch: 31 Loss: 1.4680e+08\n",
      "Epoch: 32 Loss: 1.4639e+08\n",
      "Epoch: 33 Loss: 1.4597e+08\n",
      "Epoch: 34 Loss: 1.4556e+08\n",
      "Epoch: 35 Loss: 1.4515e+08\n",
      "Epoch: 36 Loss: 1.4474e+08\n",
      "Epoch: 37 Loss: 1.4434e+08\n",
      "Epoch: 38 Loss: 1.4393e+08\n",
      "Epoch: 39 Loss: 1.4353e+08\n",
      "Epoch: 40 Loss: 1.4313e+08\n",
      "Epoch: 41 Loss: 1.4273e+08\n",
      "Epoch: 42 Loss: 1.4233e+08\n",
      "Epoch: 43 Loss: 1.4193e+08\n",
      "Epoch: 44 Loss: 1.4153e+08\n",
      "Epoch: 45 Loss: 1.4114e+08\n",
      "Epoch: 46 Loss: 1.4074e+08\n",
      "Epoch: 47 Loss: 1.4035e+08\n",
      "Epoch: 48 Loss: 1.3996e+08\n",
      "Epoch: 49 Loss: 1.3957e+08\n",
      "Epoch: 50 Loss: 1.3918e+08\n",
      "Epoch: 51 Loss: 1.3879e+08\n",
      "Epoch: 52 Loss: 1.3841e+08\n",
      "Epoch: 53 Loss: 1.3802e+08\n",
      "Epoch: 54 Loss: 1.3764e+08\n",
      "Epoch: 55 Loss: 1.3726e+08\n",
      "Epoch: 56 Loss: 1.3688e+08\n",
      "Epoch: 57 Loss: 1.3650e+08\n",
      "Epoch: 58 Loss: 1.3612e+08\n",
      "Epoch: 59 Loss: 1.3575e+08\n",
      "Epoch: 60 Loss: 1.3537e+08\n",
      "Epoch: 61 Loss: 1.3500e+08\n",
      "Epoch: 62 Loss: 1.3462e+08\n",
      "Epoch: 63 Loss: 1.3425e+08\n",
      "Epoch: 64 Loss: 1.3388e+08\n",
      "Epoch: 65 Loss: 1.3351e+08\n",
      "Epoch: 66 Loss: 1.3315e+08\n",
      "Epoch: 67 Loss: 1.3278e+08\n",
      "Epoch: 68 Loss: 1.3242e+08\n",
      "Epoch: 69 Loss: 1.3205e+08\n",
      "Epoch: 70 Loss: 1.3169e+08\n",
      "Epoch: 71 Loss: 1.3133e+08\n",
      "Epoch: 72 Loss: 1.3097e+08\n",
      "Epoch: 73 Loss: 1.3061e+08\n",
      "Epoch: 74 Loss: 1.3025e+08\n",
      "Epoch: 75 Loss: 1.2990e+08\n",
      "Epoch: 76 Loss: 1.2954e+08\n",
      "Epoch: 77 Loss: 1.2919e+08\n",
      "Epoch: 78 Loss: 1.2884e+08\n",
      "Epoch: 79 Loss: 1.2849e+08\n",
      "Epoch: 80 Loss: 1.2814e+08\n",
      "Epoch: 81 Loss: 1.2779e+08\n",
      "Epoch: 82 Loss: 1.2744e+08\n",
      "Epoch: 83 Loss: 1.2710e+08\n",
      "Epoch: 84 Loss: 1.2675e+08\n",
      "Epoch: 85 Loss: 1.2641e+08\n",
      "Epoch: 86 Loss: 1.2607e+08\n",
      "Epoch: 87 Loss: 1.2572e+08\n",
      "Epoch: 88 Loss: 1.2538e+08\n",
      "Epoch: 89 Loss: 1.2504e+08\n",
      "Epoch: 90 Loss: 1.2471e+08\n",
      "Epoch: 91 Loss: 1.2437e+08\n",
      "Epoch: 92 Loss: 1.2403e+08\n",
      "Epoch: 93 Loss: 1.2370e+08\n",
      "Epoch: 94 Loss: 1.2337e+08\n",
      "Epoch: 95 Loss: 1.2304e+08\n",
      "Epoch: 96 Loss: 1.2270e+08\n",
      "Epoch: 97 Loss: 1.2238e+08\n",
      "Epoch: 98 Loss: 1.2205e+08\n",
      "Epoch: 99 Loss: 1.2172e+08\n",
      "El error final fue: 1.2172e+08\n",
      "Epoch: 0 Loss: 1.6038e+08\n",
      "Epoch: 1 Loss: 1.5992e+08\n",
      "Epoch: 2 Loss: 1.5946e+08\n",
      "Epoch: 3 Loss: 1.5900e+08\n",
      "Epoch: 4 Loss: 1.5855e+08\n",
      "Epoch: 5 Loss: 1.5809e+08\n",
      "Epoch: 6 Loss: 1.5764e+08\n",
      "Epoch: 7 Loss: 1.5719e+08\n",
      "Epoch: 8 Loss: 1.5675e+08\n",
      "Epoch: 9 Loss: 1.5630e+08\n",
      "Epoch: 10 Loss: 1.5586e+08\n",
      "Epoch: 11 Loss: 1.5542e+08\n",
      "Epoch: 12 Loss: 1.5498e+08\n",
      "Epoch: 13 Loss: 1.5454e+08\n",
      "Epoch: 14 Loss: 1.5411e+08\n",
      "Epoch: 15 Loss: 1.5367e+08\n",
      "Epoch: 16 Loss: 1.5324e+08\n",
      "Epoch: 17 Loss: 1.5282e+08\n",
      "Epoch: 18 Loss: 1.5239e+08\n",
      "Epoch: 19 Loss: 1.5196e+08\n",
      "Epoch: 20 Loss: 1.5154e+08\n",
      "Epoch: 21 Loss: 1.5112e+08\n",
      "Epoch: 22 Loss: 1.5070e+08\n",
      "Epoch: 23 Loss: 1.5028e+08\n",
      "Epoch: 24 Loss: 1.4987e+08\n",
      "Epoch: 25 Loss: 1.4945e+08\n",
      "Epoch: 26 Loss: 1.4904e+08\n",
      "Epoch: 27 Loss: 1.4863e+08\n",
      "Epoch: 28 Loss: 1.4822e+08\n",
      "Epoch: 29 Loss: 1.4782e+08\n",
      "Epoch: 30 Loss: 1.4742e+08\n",
      "Epoch: 31 Loss: 1.4701e+08\n",
      "Epoch: 32 Loss: 1.4661e+08\n",
      "Epoch: 33 Loss: 1.4621e+08\n",
      "Epoch: 34 Loss: 1.4582e+08\n",
      "Epoch: 35 Loss: 1.4542e+08\n",
      "Epoch: 36 Loss: 1.4503e+08\n",
      "Epoch: 37 Loss: 1.4464e+08\n",
      "Epoch: 38 Loss: 1.4425e+08\n",
      "Epoch: 39 Loss: 1.4386e+08\n",
      "Epoch: 40 Loss: 1.4348e+08\n",
      "Epoch: 41 Loss: 1.4309e+08\n",
      "Epoch: 42 Loss: 1.4271e+08\n",
      "Epoch: 43 Loss: 1.4233e+08\n",
      "Epoch: 44 Loss: 1.4195e+08\n",
      "Epoch: 45 Loss: 1.4157e+08\n",
      "Epoch: 46 Loss: 1.4120e+08\n",
      "Epoch: 47 Loss: 1.4083e+08\n",
      "Epoch: 48 Loss: 1.4045e+08\n",
      "Epoch: 49 Loss: 1.4008e+08\n",
      "Epoch: 50 Loss: 1.3972e+08\n",
      "Epoch: 51 Loss: 1.3935e+08\n",
      "Epoch: 52 Loss: 1.3899e+08\n",
      "Epoch: 53 Loss: 1.3862e+08\n",
      "Epoch: 54 Loss: 1.3826e+08\n",
      "Epoch: 55 Loss: 1.3790e+08\n",
      "Epoch: 56 Loss: 1.3754e+08\n",
      "Epoch: 57 Loss: 1.3719e+08\n",
      "Epoch: 58 Loss: 1.3683e+08\n",
      "Epoch: 59 Loss: 1.3648e+08\n",
      "Epoch: 60 Loss: 1.3613e+08\n",
      "Epoch: 61 Loss: 1.3578e+08\n",
      "Epoch: 62 Loss: 1.3543e+08\n",
      "Epoch: 63 Loss: 1.3508e+08\n",
      "Epoch: 64 Loss: 1.3474e+08\n",
      "Epoch: 65 Loss: 1.3440e+08\n",
      "Epoch: 66 Loss: 1.3406e+08\n",
      "Epoch: 67 Loss: 1.3372e+08\n",
      "Epoch: 68 Loss: 1.3338e+08\n",
      "Epoch: 69 Loss: 1.3304e+08\n",
      "Epoch: 70 Loss: 1.3270e+08\n",
      "Epoch: 71 Loss: 1.3237e+08\n",
      "Epoch: 72 Loss: 1.3204e+08\n",
      "Epoch: 73 Loss: 1.3171e+08\n",
      "Epoch: 74 Loss: 1.3138e+08\n",
      "Epoch: 75 Loss: 1.3105e+08\n",
      "Epoch: 76 Loss: 1.3073e+08\n",
      "Epoch: 77 Loss: 1.3040e+08\n",
      "Epoch: 78 Loss: 1.3008e+08\n",
      "Epoch: 79 Loss: 1.2976e+08\n",
      "Epoch: 80 Loss: 1.2944e+08\n",
      "Epoch: 81 Loss: 1.2912e+08\n",
      "Epoch: 82 Loss: 1.2880e+08\n",
      "Epoch: 83 Loss: 1.2849e+08\n",
      "Epoch: 84 Loss: 1.2818e+08\n",
      "Epoch: 85 Loss: 1.2786e+08\n",
      "Epoch: 86 Loss: 1.2755e+08\n",
      "Epoch: 87 Loss: 1.2724e+08\n",
      "Epoch: 88 Loss: 1.2694e+08\n",
      "Epoch: 89 Loss: 1.2663e+08\n",
      "Epoch: 90 Loss: 1.2633e+08\n",
      "Epoch: 91 Loss: 1.2602e+08\n",
      "Epoch: 92 Loss: 1.2572e+08\n",
      "Epoch: 93 Loss: 1.2542e+08\n",
      "Epoch: 94 Loss: 1.2512e+08\n",
      "Epoch: 95 Loss: 1.2482e+08\n",
      "Epoch: 96 Loss: 1.2453e+08\n",
      "Epoch: 97 Loss: 1.2423e+08\n",
      "Epoch: 98 Loss: 1.2394e+08\n",
      "Epoch: 99 Loss: 1.2365e+08\n",
      "El error final fue: 1.2365e+08\n",
      "Epoch: 0 Loss: 1.6037e+08\n",
      "Epoch: 1 Loss: 1.5991e+08\n",
      "Epoch: 2 Loss: 1.5945e+08\n",
      "Epoch: 3 Loss: 1.5899e+08\n",
      "Epoch: 4 Loss: 1.5854e+08\n",
      "Epoch: 5 Loss: 1.5808e+08\n",
      "Epoch: 6 Loss: 1.5763e+08\n",
      "Epoch: 7 Loss: 1.5718e+08\n",
      "Epoch: 8 Loss: 1.5673e+08\n",
      "Epoch: 9 Loss: 1.5628e+08\n",
      "Epoch: 10 Loss: 1.5583e+08\n",
      "Epoch: 11 Loss: 1.5539e+08\n",
      "Epoch: 12 Loss: 1.5494e+08\n",
      "Epoch: 13 Loss: 1.5450e+08\n",
      "Epoch: 14 Loss: 1.5406e+08\n",
      "Epoch: 15 Loss: 1.5362e+08\n",
      "Epoch: 16 Loss: 1.5319e+08\n",
      "Epoch: 17 Loss: 1.5275e+08\n",
      "Epoch: 18 Loss: 1.5232e+08\n",
      "Epoch: 19 Loss: 1.5188e+08\n",
      "Epoch: 20 Loss: 1.5145e+08\n",
      "Epoch: 21 Loss: 1.5102e+08\n",
      "Epoch: 22 Loss: 1.5060e+08\n",
      "Epoch: 23 Loss: 1.5017e+08\n",
      "Epoch: 24 Loss: 1.4974e+08\n",
      "Epoch: 25 Loss: 1.4932e+08\n",
      "Epoch: 26 Loss: 1.4890e+08\n",
      "Epoch: 27 Loss: 1.4848e+08\n",
      "Epoch: 28 Loss: 1.4806e+08\n",
      "Epoch: 29 Loss: 1.4764e+08\n",
      "Epoch: 30 Loss: 1.4723e+08\n",
      "Epoch: 31 Loss: 1.4682e+08\n",
      "Epoch: 32 Loss: 1.4640e+08\n",
      "Epoch: 33 Loss: 1.4599e+08\n",
      "Epoch: 34 Loss: 1.4558e+08\n",
      "Epoch: 35 Loss: 1.4517e+08\n",
      "Epoch: 36 Loss: 1.4477e+08\n",
      "Epoch: 37 Loss: 1.4436e+08\n",
      "Epoch: 38 Loss: 1.4396e+08\n",
      "Epoch: 39 Loss: 1.4356e+08\n",
      "Epoch: 40 Loss: 1.4316e+08\n",
      "Epoch: 41 Loss: 1.4276e+08\n",
      "Epoch: 42 Loss: 1.4236e+08\n",
      "Epoch: 43 Loss: 1.4196e+08\n",
      "Epoch: 44 Loss: 1.4157e+08\n",
      "Epoch: 45 Loss: 1.4117e+08\n",
      "Epoch: 46 Loss: 1.4078e+08\n",
      "Epoch: 47 Loss: 1.4039e+08\n",
      "Epoch: 48 Loss: 1.4000e+08\n",
      "Epoch: 49 Loss: 1.3962e+08\n",
      "Epoch: 50 Loss: 1.3923e+08\n",
      "Epoch: 51 Loss: 1.3884e+08\n",
      "Epoch: 52 Loss: 1.3846e+08\n",
      "Epoch: 53 Loss: 1.3808e+08\n",
      "Epoch: 54 Loss: 1.3770e+08\n",
      "Epoch: 55 Loss: 1.3732e+08\n",
      "Epoch: 56 Loss: 1.3694e+08\n",
      "Epoch: 57 Loss: 1.3656e+08\n",
      "Epoch: 58 Loss: 1.3619e+08\n",
      "Epoch: 59 Loss: 1.3581e+08\n",
      "Epoch: 60 Loss: 1.3544e+08\n",
      "Epoch: 61 Loss: 1.3507e+08\n",
      "Epoch: 62 Loss: 1.3470e+08\n",
      "Epoch: 63 Loss: 1.3433e+08\n",
      "Epoch: 64 Loss: 1.3396e+08\n",
      "Epoch: 65 Loss: 1.3360e+08\n",
      "Epoch: 66 Loss: 1.3323e+08\n",
      "Epoch: 67 Loss: 1.3287e+08\n",
      "Epoch: 68 Loss: 1.3251e+08\n",
      "Epoch: 69 Loss: 1.3215e+08\n",
      "Epoch: 70 Loss: 1.3179e+08\n",
      "Epoch: 71 Loss: 1.3143e+08\n",
      "Epoch: 72 Loss: 1.3107e+08\n",
      "Epoch: 73 Loss: 1.3072e+08\n",
      "Epoch: 74 Loss: 1.3036e+08\n",
      "Epoch: 75 Loss: 1.3001e+08\n",
      "Epoch: 76 Loss: 1.2966e+08\n",
      "Epoch: 77 Loss: 1.2931e+08\n",
      "Epoch: 78 Loss: 1.2896e+08\n",
      "Epoch: 79 Loss: 1.2861e+08\n",
      "Epoch: 80 Loss: 1.2826e+08\n",
      "Epoch: 81 Loss: 1.2792e+08\n",
      "Epoch: 82 Loss: 1.2757e+08\n",
      "Epoch: 83 Loss: 1.2723e+08\n",
      "Epoch: 84 Loss: 1.2689e+08\n",
      "Epoch: 85 Loss: 1.2655e+08\n",
      "Epoch: 86 Loss: 1.2621e+08\n",
      "Epoch: 87 Loss: 1.2587e+08\n",
      "Epoch: 88 Loss: 1.2553e+08\n",
      "Epoch: 89 Loss: 1.2520e+08\n",
      "Epoch: 90 Loss: 1.2486e+08\n",
      "Epoch: 91 Loss: 1.2453e+08\n",
      "Epoch: 92 Loss: 1.2420e+08\n",
      "Epoch: 93 Loss: 1.2387e+08\n",
      "Epoch: 94 Loss: 1.2354e+08\n",
      "Epoch: 95 Loss: 1.2321e+08\n",
      "Epoch: 96 Loss: 1.2288e+08\n",
      "Epoch: 97 Loss: 1.2256e+08\n",
      "Epoch: 98 Loss: 1.2223e+08\n",
      "Epoch: 99 Loss: 1.2191e+08\n",
      "El error final fue: 1.2191e+08\n",
      "Epoch: 0 Loss: 1.6032e+08\n",
      "Epoch: 1 Loss: 1.5757e+08\n",
      "Epoch: 2 Loss: 1.5487e+08\n",
      "Epoch: 3 Loss: 1.5224e+08\n",
      "Epoch: 4 Loss: 1.4968e+08\n",
      "Epoch: 5 Loss: 1.4717e+08\n",
      "Epoch: 6 Loss: 1.4472e+08\n",
      "Epoch: 7 Loss: 1.4233e+08\n",
      "Epoch: 8 Loss: 1.3999e+08\n",
      "Epoch: 9 Loss: 1.3771e+08\n",
      "Epoch: 10 Loss: 1.3548e+08\n",
      "Epoch: 11 Loss: 1.3330e+08\n",
      "Epoch: 12 Loss: 1.3117e+08\n",
      "Epoch: 13 Loss: 1.2909e+08\n",
      "Epoch: 14 Loss: 1.2706e+08\n",
      "Epoch: 15 Loss: 1.2507e+08\n",
      "Epoch: 16 Loss: 1.2313e+08\n",
      "Epoch: 17 Loss: 1.2124e+08\n",
      "Epoch: 18 Loss: 1.1939e+08\n",
      "Epoch: 19 Loss: 1.1758e+08\n",
      "Epoch: 20 Loss: 1.1581e+08\n",
      "Epoch: 21 Loss: 1.1408e+08\n",
      "Epoch: 22 Loss: 1.1239e+08\n",
      "Epoch: 23 Loss: 1.1074e+08\n",
      "Epoch: 24 Loss: 1.0913e+08\n",
      "Epoch: 25 Loss: 1.0755e+08\n",
      "Epoch: 26 Loss: 1.0601e+08\n",
      "Epoch: 27 Loss: 1.0450e+08\n",
      "Epoch: 28 Loss: 1.0303e+08\n",
      "Epoch: 29 Loss: 1.0159e+08\n",
      "Epoch: 30 Loss: 1.0018e+08\n",
      "Epoch: 31 Loss: 9.8802e+07\n",
      "Epoch: 32 Loss: 9.7456e+07\n",
      "Epoch: 33 Loss: 9.6140e+07\n",
      "Epoch: 34 Loss: 9.4852e+07\n",
      "Epoch: 35 Loss: 9.3594e+07\n",
      "Epoch: 36 Loss: 9.2363e+07\n",
      "Epoch: 37 Loss: 9.1159e+07\n",
      "Epoch: 38 Loss: 8.9981e+07\n",
      "Epoch: 39 Loss: 8.8829e+07\n",
      "Epoch: 40 Loss: 8.7703e+07\n",
      "Epoch: 41 Loss: 8.6600e+07\n",
      "Epoch: 42 Loss: 8.5522e+07\n",
      "Epoch: 43 Loss: 8.4468e+07\n",
      "Epoch: 44 Loss: 8.3436e+07\n",
      "Epoch: 45 Loss: 8.2426e+07\n",
      "Epoch: 46 Loss: 8.1438e+07\n",
      "Epoch: 47 Loss: 8.0471e+07\n",
      "Epoch: 48 Loss: 7.9525e+07\n",
      "Epoch: 49 Loss: 7.8599e+07\n",
      "Epoch: 50 Loss: 7.7693e+07\n",
      "Epoch: 51 Loss: 7.6806e+07\n",
      "Epoch: 52 Loss: 7.5937e+07\n",
      "Epoch: 53 Loss: 7.5087e+07\n",
      "Epoch: 54 Loss: 7.4255e+07\n",
      "Epoch: 55 Loss: 7.3441e+07\n",
      "Epoch: 56 Loss: 7.2643e+07\n",
      "Epoch: 57 Loss: 7.1862e+07\n",
      "Epoch: 58 Loss: 7.1097e+07\n",
      "Epoch: 59 Loss: 7.0348e+07\n",
      "Epoch: 60 Loss: 6.9615e+07\n",
      "Epoch: 61 Loss: 6.8896e+07\n",
      "Epoch: 62 Loss: 6.8193e+07\n",
      "Epoch: 63 Loss: 6.7503e+07\n",
      "Epoch: 64 Loss: 6.6828e+07\n",
      "Epoch: 65 Loss: 6.6166e+07\n",
      "Epoch: 66 Loss: 6.5518e+07\n",
      "Epoch: 67 Loss: 6.4883e+07\n",
      "Epoch: 68 Loss: 6.4261e+07\n",
      "Epoch: 69 Loss: 6.3651e+07\n",
      "Epoch: 70 Loss: 6.3053e+07\n",
      "Epoch: 71 Loss: 6.2467e+07\n",
      "Epoch: 72 Loss: 6.1892e+07\n",
      "Epoch: 73 Loss: 6.1329e+07\n",
      "Epoch: 74 Loss: 6.0777e+07\n",
      "Epoch: 75 Loss: 6.0236e+07\n",
      "Epoch: 76 Loss: 5.9705e+07\n",
      "Epoch: 77 Loss: 5.9185e+07\n",
      "Epoch: 78 Loss: 5.8674e+07\n",
      "Epoch: 79 Loss: 5.8173e+07\n",
      "Epoch: 80 Loss: 5.7682e+07\n",
      "Epoch: 81 Loss: 5.7201e+07\n",
      "Epoch: 82 Loss: 5.6728e+07\n",
      "Epoch: 83 Loss: 5.6264e+07\n",
      "Epoch: 84 Loss: 5.5810e+07\n",
      "Epoch: 85 Loss: 5.5363e+07\n",
      "Epoch: 86 Loss: 5.4925e+07\n",
      "Epoch: 87 Loss: 5.4495e+07\n",
      "Epoch: 88 Loss: 5.4073e+07\n",
      "Epoch: 89 Loss: 5.3658e+07\n",
      "Epoch: 90 Loss: 5.3252e+07\n",
      "Epoch: 91 Loss: 5.2852e+07\n",
      "Epoch: 92 Loss: 5.2460e+07\n",
      "Epoch: 93 Loss: 5.2075e+07\n",
      "Epoch: 94 Loss: 5.1697e+07\n",
      "Epoch: 95 Loss: 5.1325e+07\n",
      "Epoch: 96 Loss: 5.0960e+07\n",
      "Epoch: 97 Loss: 5.0602e+07\n",
      "Epoch: 98 Loss: 5.0250e+07\n",
      "Epoch: 99 Loss: 4.9904e+07\n",
      "El error final fue: 4.9904e+07\n",
      "Epoch: 0 Loss: 1.6034e+08\n",
      "Epoch: 1 Loss: 1.5759e+08\n",
      "Epoch: 2 Loss: 1.5492e+08\n",
      "Epoch: 3 Loss: 1.5233e+08\n",
      "Epoch: 4 Loss: 1.4981e+08\n",
      "Epoch: 5 Loss: 1.4737e+08\n",
      "Epoch: 6 Loss: 1.4500e+08\n",
      "Epoch: 7 Loss: 1.4269e+08\n",
      "Epoch: 8 Loss: 1.4046e+08\n",
      "Epoch: 9 Loss: 1.3829e+08\n",
      "Epoch: 10 Loss: 1.3618e+08\n",
      "Epoch: 11 Loss: 1.3414e+08\n",
      "Epoch: 12 Loss: 1.3215e+08\n",
      "Epoch: 13 Loss: 1.3023e+08\n",
      "Epoch: 14 Loss: 1.2836e+08\n",
      "Epoch: 15 Loss: 1.2655e+08\n",
      "Epoch: 16 Loss: 1.2479e+08\n",
      "Epoch: 17 Loss: 1.2309e+08\n",
      "Epoch: 18 Loss: 1.2144e+08\n",
      "Epoch: 19 Loss: 1.1983e+08\n",
      "Epoch: 20 Loss: 1.1828e+08\n",
      "Epoch: 21 Loss: 1.1677e+08\n",
      "Epoch: 22 Loss: 1.1531e+08\n",
      "Epoch: 23 Loss: 1.1389e+08\n",
      "Epoch: 24 Loss: 1.1252e+08\n",
      "Epoch: 25 Loss: 1.1119e+08\n",
      "Epoch: 26 Loss: 1.0990e+08\n",
      "Epoch: 27 Loss: 1.0865e+08\n",
      "Epoch: 28 Loss: 1.0744e+08\n",
      "Epoch: 29 Loss: 1.0627e+08\n",
      "Epoch: 30 Loss: 1.0514e+08\n",
      "Epoch: 31 Loss: 1.0404e+08\n",
      "Epoch: 32 Loss: 1.0297e+08\n",
      "Epoch: 33 Loss: 1.0194e+08\n",
      "Epoch: 34 Loss: 1.0095e+08\n",
      "Epoch: 35 Loss: 9.9981e+07\n",
      "Epoch: 36 Loss: 9.9047e+07\n",
      "Epoch: 37 Loss: 9.8144e+07\n",
      "Epoch: 38 Loss: 9.7270e+07\n",
      "Epoch: 39 Loss: 9.6426e+07\n",
      "Epoch: 40 Loss: 9.5609e+07\n",
      "Epoch: 41 Loss: 9.4819e+07\n",
      "Epoch: 42 Loss: 9.4056e+07\n",
      "Epoch: 43 Loss: 9.3318e+07\n",
      "Epoch: 44 Loss: 9.2606e+07\n",
      "Epoch: 45 Loss: 9.1917e+07\n",
      "Epoch: 46 Loss: 9.1253e+07\n",
      "Epoch: 47 Loss: 9.0611e+07\n",
      "Epoch: 48 Loss: 8.9992e+07\n",
      "Epoch: 49 Loss: 8.9394e+07\n",
      "Epoch: 50 Loss: 8.8818e+07\n",
      "Epoch: 51 Loss: 8.8262e+07\n",
      "Epoch: 52 Loss: 8.7725e+07\n",
      "Epoch: 53 Loss: 8.7209e+07\n",
      "Epoch: 54 Loss: 8.6711e+07\n",
      "Epoch: 55 Loss: 8.6231e+07\n",
      "Epoch: 56 Loss: 8.5770e+07\n",
      "Epoch: 57 Loss: 8.5325e+07\n",
      "Epoch: 58 Loss: 8.4897e+07\n",
      "Epoch: 59 Loss: 8.4486e+07\n",
      "Epoch: 60 Loss: 8.4091e+07\n",
      "Epoch: 61 Loss: 8.3710e+07\n",
      "Epoch: 62 Loss: 8.3345e+07\n",
      "Epoch: 63 Loss: 8.2995e+07\n",
      "Epoch: 64 Loss: 8.2658e+07\n",
      "Epoch: 65 Loss: 8.2335e+07\n",
      "Epoch: 66 Loss: 8.2026e+07\n",
      "Epoch: 67 Loss: 8.1730e+07\n",
      "Epoch: 68 Loss: 8.1446e+07\n",
      "Epoch: 69 Loss: 8.1174e+07\n",
      "Epoch: 70 Loss: 8.0914e+07\n",
      "Epoch: 71 Loss: 8.0666e+07\n",
      "Epoch: 72 Loss: 8.0429e+07\n",
      "Epoch: 73 Loss: 8.0202e+07\n",
      "Epoch: 74 Loss: 7.9987e+07\n",
      "Epoch: 75 Loss: 7.9781e+07\n",
      "Epoch: 76 Loss: 7.9586e+07\n",
      "Epoch: 77 Loss: 7.9400e+07\n",
      "Epoch: 78 Loss: 7.9223e+07\n",
      "Epoch: 79 Loss: 7.9056e+07\n",
      "Epoch: 80 Loss: 7.8897e+07\n",
      "Epoch: 81 Loss: 7.8747e+07\n",
      "Epoch: 82 Loss: 7.8605e+07\n",
      "Epoch: 83 Loss: 7.8472e+07\n",
      "Epoch: 84 Loss: 7.8346e+07\n",
      "Epoch: 85 Loss: 7.8227e+07\n",
      "Epoch: 86 Loss: 7.8117e+07\n",
      "Epoch: 87 Loss: 7.8013e+07\n",
      "Epoch: 88 Loss: 7.7916e+07\n",
      "Epoch: 89 Loss: 7.7826e+07\n",
      "Epoch: 90 Loss: 7.7742e+07\n",
      "Epoch: 91 Loss: 7.7665e+07\n",
      "Epoch: 92 Loss: 7.7594e+07\n",
      "Epoch: 93 Loss: 7.7528e+07\n",
      "Epoch: 94 Loss: 7.7469e+07\n",
      "Epoch: 95 Loss: 7.7415e+07\n",
      "Epoch: 96 Loss: 7.7367e+07\n",
      "Epoch: 97 Loss: 7.7324e+07\n",
      "Epoch: 98 Loss: 7.7285e+07\n",
      "Epoch: 99 Loss: 7.7252e+07\n",
      "El error final fue: 7.7252e+07\n",
      "Epoch: 0 Loss: 1.6032e+08\n",
      "Epoch: 1 Loss: 1.5756e+08\n",
      "Epoch: 2 Loss: 1.5487e+08\n",
      "Epoch: 3 Loss: 1.5225e+08\n",
      "Epoch: 4 Loss: 1.4969e+08\n",
      "Epoch: 5 Loss: 1.4719e+08\n",
      "Epoch: 6 Loss: 1.4475e+08\n",
      "Epoch: 7 Loss: 1.4236e+08\n",
      "Epoch: 8 Loss: 1.4004e+08\n",
      "Epoch: 9 Loss: 1.3776e+08\n",
      "Epoch: 10 Loss: 1.3555e+08\n",
      "Epoch: 11 Loss: 1.3338e+08\n",
      "Epoch: 12 Loss: 1.3127e+08\n",
      "Epoch: 13 Loss: 1.2920e+08\n",
      "Epoch: 14 Loss: 1.2719e+08\n",
      "Epoch: 15 Loss: 1.2522e+08\n",
      "Epoch: 16 Loss: 1.2330e+08\n",
      "Epoch: 17 Loss: 1.2142e+08\n",
      "Epoch: 18 Loss: 1.1959e+08\n",
      "Epoch: 19 Loss: 1.1780e+08\n",
      "Epoch: 20 Loss: 1.1606e+08\n",
      "Epoch: 21 Loss: 1.1435e+08\n",
      "Epoch: 22 Loss: 1.1268e+08\n",
      "Epoch: 23 Loss: 1.1106e+08\n",
      "Epoch: 24 Loss: 1.0947e+08\n",
      "Epoch: 25 Loss: 1.0791e+08\n",
      "Epoch: 26 Loss: 1.0640e+08\n",
      "Epoch: 27 Loss: 1.0492e+08\n",
      "Epoch: 28 Loss: 1.0347e+08\n",
      "Epoch: 29 Loss: 1.0206e+08\n",
      "Epoch: 30 Loss: 1.0068e+08\n",
      "Epoch: 31 Loss: 9.9326e+07\n",
      "Epoch: 32 Loss: 9.8008e+07\n",
      "Epoch: 33 Loss: 9.6721e+07\n",
      "Epoch: 34 Loss: 9.5463e+07\n",
      "Epoch: 35 Loss: 9.4233e+07\n",
      "Epoch: 36 Loss: 9.3032e+07\n",
      "Epoch: 37 Loss: 9.1858e+07\n",
      "Epoch: 38 Loss: 9.0711e+07\n",
      "Epoch: 39 Loss: 8.9590e+07\n",
      "Epoch: 40 Loss: 8.8495e+07\n",
      "Epoch: 41 Loss: 8.7424e+07\n",
      "Epoch: 42 Loss: 8.6377e+07\n",
      "Epoch: 43 Loss: 8.5354e+07\n",
      "Epoch: 44 Loss: 8.4354e+07\n",
      "Epoch: 45 Loss: 8.3377e+07\n",
      "Epoch: 46 Loss: 8.2421e+07\n",
      "Epoch: 47 Loss: 8.1487e+07\n",
      "Epoch: 48 Loss: 8.0574e+07\n",
      "Epoch: 49 Loss: 7.9680e+07\n",
      "Epoch: 50 Loss: 7.8807e+07\n",
      "Epoch: 51 Loss: 7.7953e+07\n",
      "Epoch: 52 Loss: 7.7118e+07\n",
      "Epoch: 53 Loss: 7.6302e+07\n",
      "Epoch: 54 Loss: 7.5503e+07\n",
      "Epoch: 55 Loss: 7.4722e+07\n",
      "Epoch: 56 Loss: 7.3958e+07\n",
      "Epoch: 57 Loss: 7.3211e+07\n",
      "Epoch: 58 Loss: 7.2480e+07\n",
      "Epoch: 59 Loss: 7.1764e+07\n",
      "Epoch: 60 Loss: 7.1065e+07\n",
      "Epoch: 61 Loss: 7.0380e+07\n",
      "Epoch: 62 Loss: 6.9710e+07\n",
      "Epoch: 63 Loss: 6.9055e+07\n",
      "Epoch: 64 Loss: 6.8413e+07\n",
      "Epoch: 65 Loss: 6.7786e+07\n",
      "Epoch: 66 Loss: 6.7171e+07\n",
      "Epoch: 67 Loss: 6.6570e+07\n",
      "Epoch: 68 Loss: 6.5982e+07\n",
      "Epoch: 69 Loss: 6.5405e+07\n",
      "Epoch: 70 Loss: 6.4841e+07\n",
      "Epoch: 71 Loss: 6.4289e+07\n",
      "Epoch: 72 Loss: 6.3749e+07\n",
      "Epoch: 73 Loss: 6.3219e+07\n",
      "Epoch: 74 Loss: 6.2701e+07\n",
      "Epoch: 75 Loss: 6.2193e+07\n",
      "Epoch: 76 Loss: 6.1696e+07\n",
      "Epoch: 77 Loss: 6.1209e+07\n",
      "Epoch: 78 Loss: 6.0732e+07\n",
      "Epoch: 79 Loss: 6.0264e+07\n",
      "Epoch: 80 Loss: 5.9807e+07\n",
      "Epoch: 81 Loss: 5.9358e+07\n",
      "Epoch: 82 Loss: 5.8919e+07\n",
      "Epoch: 83 Loss: 5.8488e+07\n",
      "Epoch: 84 Loss: 5.8066e+07\n",
      "Epoch: 85 Loss: 5.7652e+07\n",
      "Epoch: 86 Loss: 5.7247e+07\n",
      "Epoch: 87 Loss: 5.6849e+07\n",
      "Epoch: 88 Loss: 5.6460e+07\n",
      "Epoch: 89 Loss: 5.6078e+07\n",
      "Epoch: 90 Loss: 5.5704e+07\n",
      "Epoch: 91 Loss: 5.5336e+07\n",
      "Epoch: 92 Loss: 5.4976e+07\n",
      "Epoch: 93 Loss: 5.4623e+07\n",
      "Epoch: 94 Loss: 5.4277e+07\n",
      "Epoch: 95 Loss: 5.3937e+07\n",
      "Epoch: 96 Loss: 5.3604e+07\n",
      "Epoch: 97 Loss: 5.3277e+07\n",
      "Epoch: 98 Loss: 5.2956e+07\n",
      "Epoch: 99 Loss: 5.2641e+07\n",
      "El error final fue: 5.2641e+07\n",
      "Epoch: 0 Loss: 1.6024e+08\n",
      "Epoch: 1 Loss: 1.5251e+08\n",
      "Epoch: 2 Loss: 1.4523e+08\n",
      "Epoch: 3 Loss: 1.3835e+08\n",
      "Epoch: 4 Loss: 1.3187e+08\n",
      "Epoch: 5 Loss: 1.2575e+08\n",
      "Epoch: 6 Loss: 1.1997e+08\n",
      "Epoch: 7 Loss: 1.1453e+08\n",
      "Epoch: 8 Loss: 1.0938e+08\n",
      "Epoch: 9 Loss: 1.0452e+08\n",
      "Epoch: 10 Loss: 9.9937e+07\n",
      "Epoch: 11 Loss: 9.5604e+07\n",
      "Epoch: 12 Loss: 9.1511e+07\n",
      "Epoch: 13 Loss: 8.7644e+07\n",
      "Epoch: 14 Loss: 8.3990e+07\n",
      "Epoch: 15 Loss: 8.0536e+07\n",
      "Epoch: 16 Loss: 7.7270e+07\n",
      "Epoch: 17 Loss: 7.4182e+07\n",
      "Epoch: 18 Loss: 7.1262e+07\n",
      "Epoch: 19 Loss: 6.8499e+07\n",
      "Epoch: 20 Loss: 6.5885e+07\n",
      "Epoch: 21 Loss: 6.3411e+07\n",
      "Epoch: 22 Loss: 6.1070e+07\n",
      "Epoch: 23 Loss: 5.8853e+07\n",
      "Epoch: 24 Loss: 5.6754e+07\n",
      "Epoch: 25 Loss: 5.4765e+07\n",
      "Epoch: 26 Loss: 5.2881e+07\n",
      "Epoch: 27 Loss: 5.1096e+07\n",
      "Epoch: 28 Loss: 4.9404e+07\n",
      "Epoch: 29 Loss: 4.7800e+07\n",
      "Epoch: 30 Loss: 4.6279e+07\n",
      "Epoch: 31 Loss: 4.4836e+07\n",
      "Epoch: 32 Loss: 4.3467e+07\n",
      "Epoch: 33 Loss: 4.2168e+07\n",
      "Epoch: 34 Loss: 4.0935e+07\n",
      "Epoch: 35 Loss: 3.9764e+07\n",
      "Epoch: 36 Loss: 3.8652e+07\n",
      "Epoch: 37 Loss: 3.7595e+07\n",
      "Epoch: 38 Loss: 3.6591e+07\n",
      "Epoch: 39 Loss: 3.5637e+07\n",
      "Epoch: 40 Loss: 3.4730e+07\n",
      "Epoch: 41 Loss: 3.3867e+07\n",
      "Epoch: 42 Loss: 3.3046e+07\n",
      "Epoch: 43 Loss: 3.2264e+07\n",
      "Epoch: 44 Loss: 3.1521e+07\n",
      "Epoch: 45 Loss: 3.0813e+07\n",
      "Epoch: 46 Loss: 3.0138e+07\n",
      "Epoch: 47 Loss: 2.9495e+07\n",
      "Epoch: 48 Loss: 2.8883e+07\n",
      "Epoch: 49 Loss: 2.8299e+07\n",
      "Epoch: 50 Loss: 2.7742e+07\n",
      "Epoch: 51 Loss: 2.7210e+07\n",
      "Epoch: 52 Loss: 2.6703e+07\n",
      "Epoch: 53 Loss: 2.6219e+07\n",
      "Epoch: 54 Loss: 2.5757e+07\n",
      "Epoch: 55 Loss: 2.5315e+07\n",
      "Epoch: 56 Loss: 2.4893e+07\n",
      "Epoch: 57 Loss: 2.4489e+07\n",
      "Epoch: 58 Loss: 2.4103e+07\n",
      "Epoch: 59 Loss: 2.3734e+07\n",
      "Epoch: 60 Loss: 2.3380e+07\n",
      "Epoch: 61 Loss: 2.3042e+07\n",
      "Epoch: 62 Loss: 2.2718e+07\n",
      "Epoch: 63 Loss: 2.2407e+07\n",
      "Epoch: 64 Loss: 2.2110e+07\n",
      "Epoch: 65 Loss: 2.1824e+07\n",
      "Epoch: 66 Loss: 2.1551e+07\n",
      "Epoch: 67 Loss: 2.1288e+07\n",
      "Epoch: 68 Loss: 2.1036e+07\n",
      "Epoch: 69 Loss: 2.0793e+07\n",
      "Epoch: 70 Loss: 2.0561e+07\n",
      "Epoch: 71 Loss: 2.0337e+07\n",
      "Epoch: 72 Loss: 2.0122e+07\n",
      "Epoch: 73 Loss: 1.9915e+07\n",
      "Epoch: 74 Loss: 1.9716e+07\n",
      "Epoch: 75 Loss: 1.9525e+07\n",
      "Epoch: 76 Loss: 1.9340e+07\n",
      "Epoch: 77 Loss: 1.9162e+07\n",
      "Epoch: 78 Loss: 1.8991e+07\n",
      "Epoch: 79 Loss: 1.8825e+07\n",
      "Epoch: 80 Loss: 1.8666e+07\n",
      "Epoch: 81 Loss: 1.8512e+07\n",
      "Epoch: 82 Loss: 1.8364e+07\n",
      "Epoch: 83 Loss: 1.8220e+07\n",
      "Epoch: 84 Loss: 1.8082e+07\n",
      "Epoch: 85 Loss: 1.7948e+07\n",
      "Epoch: 86 Loss: 1.7818e+07\n",
      "Epoch: 87 Loss: 1.7693e+07\n",
      "Epoch: 88 Loss: 1.7571e+07\n",
      "Epoch: 89 Loss: 1.7454e+07\n",
      "Epoch: 90 Loss: 1.7340e+07\n",
      "Epoch: 91 Loss: 1.7230e+07\n",
      "Epoch: 92 Loss: 1.7123e+07\n",
      "Epoch: 93 Loss: 1.7019e+07\n",
      "Epoch: 94 Loss: 1.6918e+07\n",
      "Epoch: 95 Loss: 1.6821e+07\n",
      "Epoch: 96 Loss: 1.6726e+07\n",
      "Epoch: 97 Loss: 1.6634e+07\n",
      "Epoch: 98 Loss: 1.6544e+07\n",
      "Epoch: 99 Loss: 1.6457e+07\n",
      "El error final fue: 1.6457e+07\n",
      "Epoch: 0 Loss: 1.6023e+08\n",
      "Epoch: 1 Loss: 1.5260e+08\n",
      "Epoch: 2 Loss: 1.4556e+08\n",
      "Epoch: 3 Loss: 1.3909e+08\n",
      "Epoch: 4 Loss: 1.3314e+08\n",
      "Epoch: 5 Loss: 1.2768e+08\n",
      "Epoch: 6 Loss: 1.2268e+08\n",
      "Epoch: 7 Loss: 1.1810e+08\n",
      "Epoch: 8 Loss: 1.1392e+08\n",
      "Epoch: 9 Loss: 1.1011e+08\n",
      "Epoch: 10 Loss: 1.0664e+08\n",
      "Epoch: 11 Loss: 1.0350e+08\n",
      "Epoch: 12 Loss: 1.0065e+08\n",
      "Epoch: 13 Loss: 9.8082e+07\n",
      "Epoch: 14 Loss: 9.5772e+07\n",
      "Epoch: 15 Loss: 9.3702e+07\n",
      "Epoch: 16 Loss: 9.1856e+07\n",
      "Epoch: 17 Loss: 9.0218e+07\n",
      "Epoch: 18 Loss: 8.8773e+07\n",
      "Epoch: 19 Loss: 8.7507e+07\n",
      "Epoch: 20 Loss: 8.6408e+07\n",
      "Epoch: 21 Loss: 8.5462e+07\n",
      "Epoch: 22 Loss: 8.4660e+07\n",
      "Epoch: 23 Loss: 8.3989e+07\n",
      "Epoch: 24 Loss: 8.3441e+07\n",
      "Epoch: 25 Loss: 8.3007e+07\n",
      "Epoch: 26 Loss: 8.2676e+07\n",
      "Epoch: 27 Loss: 8.2443e+07\n",
      "Epoch: 28 Loss: 8.2298e+07\n",
      "Epoch: 29 Loss: 8.2236e+07\n",
      "Epoch: 30 Loss: 8.2249e+07\n",
      "Epoch: 31 Loss: 8.2331e+07\n",
      "Epoch: 32 Loss: 8.2477e+07\n",
      "Epoch: 33 Loss: 8.2681e+07\n",
      "Epoch: 34 Loss: 8.2939e+07\n",
      "Epoch: 35 Loss: 8.3246e+07\n",
      "Epoch: 36 Loss: 8.3597e+07\n",
      "Epoch: 37 Loss: 8.3989e+07\n",
      "Epoch: 38 Loss: 8.4417e+07\n",
      "La funcion de perdida no ha disminuido, parando despues de 39 epocas. el error es: 8.4879e+07\n",
      "El error final fue: 8.4879e+07\n",
      "Epoch: 0 Loss: 1.6022e+08\n",
      "Epoch: 1 Loss: 1.5250e+08\n",
      "Epoch: 2 Loss: 1.4524e+08\n",
      "Epoch: 3 Loss: 1.3841e+08\n",
      "Epoch: 4 Loss: 1.3198e+08\n",
      "Epoch: 5 Loss: 1.2592e+08\n",
      "Epoch: 6 Loss: 1.2023e+08\n",
      "Epoch: 7 Loss: 1.1487e+08\n",
      "Epoch: 8 Loss: 1.0982e+08\n",
      "Epoch: 9 Loss: 1.0507e+08\n",
      "Epoch: 10 Loss: 1.0059e+08\n",
      "Epoch: 11 Loss: 9.6381e+07\n",
      "Epoch: 12 Loss: 9.2413e+07\n",
      "Epoch: 13 Loss: 8.8677e+07\n",
      "Epoch: 14 Loss: 8.5157e+07\n",
      "Epoch: 15 Loss: 8.1842e+07\n",
      "Epoch: 16 Loss: 7.8719e+07\n",
      "Epoch: 17 Loss: 7.5776e+07\n",
      "Epoch: 18 Loss: 7.3004e+07\n",
      "Epoch: 19 Loss: 7.0391e+07\n",
      "Epoch: 20 Loss: 6.7929e+07\n",
      "Epoch: 21 Loss: 6.5609e+07\n",
      "Epoch: 22 Loss: 6.3422e+07\n",
      "Epoch: 23 Loss: 6.1360e+07\n",
      "Epoch: 24 Loss: 5.9416e+07\n",
      "Epoch: 25 Loss: 5.7584e+07\n",
      "Epoch: 26 Loss: 5.5855e+07\n",
      "Epoch: 27 Loss: 5.4226e+07\n",
      "Epoch: 28 Loss: 5.2689e+07\n",
      "Epoch: 29 Loss: 5.1239e+07\n",
      "Epoch: 30 Loss: 4.9872e+07\n",
      "Epoch: 31 Loss: 4.8582e+07\n",
      "Epoch: 32 Loss: 4.7364e+07\n",
      "Epoch: 33 Loss: 4.6216e+07\n",
      "Epoch: 34 Loss: 4.5132e+07\n",
      "Epoch: 35 Loss: 4.4109e+07\n",
      "Epoch: 36 Loss: 4.3143e+07\n",
      "Epoch: 37 Loss: 4.2232e+07\n",
      "Epoch: 38 Loss: 4.1371e+07\n",
      "Epoch: 39 Loss: 4.0559e+07\n",
      "Epoch: 40 Loss: 3.9792e+07\n",
      "Epoch: 41 Loss: 3.9067e+07\n",
      "Epoch: 42 Loss: 3.8383e+07\n",
      "Epoch: 43 Loss: 3.7736e+07\n",
      "Epoch: 44 Loss: 3.7125e+07\n",
      "Epoch: 45 Loss: 3.6548e+07\n",
      "Epoch: 46 Loss: 3.6003e+07\n",
      "Epoch: 47 Loss: 3.5487e+07\n",
      "Epoch: 48 Loss: 3.5000e+07\n",
      "Epoch: 49 Loss: 3.4540e+07\n",
      "Epoch: 50 Loss: 3.4104e+07\n",
      "Epoch: 51 Loss: 3.3692e+07\n",
      "Epoch: 52 Loss: 3.3303e+07\n",
      "Epoch: 53 Loss: 3.2935e+07\n",
      "Epoch: 54 Loss: 3.2586e+07\n",
      "Epoch: 55 Loss: 3.2257e+07\n",
      "Epoch: 56 Loss: 3.1945e+07\n",
      "Epoch: 57 Loss: 3.1650e+07\n",
      "Epoch: 58 Loss: 3.1370e+07\n",
      "Epoch: 59 Loss: 3.1106e+07\n",
      "Epoch: 60 Loss: 3.0856e+07\n",
      "Epoch: 61 Loss: 3.0618e+07\n",
      "Epoch: 62 Loss: 3.0394e+07\n",
      "Epoch: 63 Loss: 3.0181e+07\n",
      "Epoch: 64 Loss: 2.9980e+07\n",
      "Epoch: 65 Loss: 2.9789e+07\n",
      "Epoch: 66 Loss: 2.9608e+07\n",
      "Epoch: 67 Loss: 2.9436e+07\n",
      "Epoch: 68 Loss: 2.9273e+07\n",
      "Epoch: 69 Loss: 2.9119e+07\n",
      "Epoch: 70 Loss: 2.8973e+07\n",
      "Epoch: 71 Loss: 2.8834e+07\n",
      "Epoch: 72 Loss: 2.8702e+07\n",
      "Epoch: 73 Loss: 2.8577e+07\n",
      "Epoch: 74 Loss: 2.8458e+07\n",
      "Epoch: 75 Loss: 2.8346e+07\n",
      "Epoch: 76 Loss: 2.8239e+07\n",
      "Epoch: 77 Loss: 2.8137e+07\n",
      "Epoch: 78 Loss: 2.8040e+07\n",
      "Epoch: 79 Loss: 2.7949e+07\n",
      "Epoch: 80 Loss: 2.7861e+07\n",
      "Epoch: 81 Loss: 2.7779e+07\n",
      "Epoch: 82 Loss: 2.7700e+07\n",
      "Epoch: 83 Loss: 2.7625e+07\n",
      "Epoch: 84 Loss: 2.7553e+07\n",
      "Epoch: 85 Loss: 2.7485e+07\n",
      "Epoch: 86 Loss: 2.7420e+07\n",
      "Epoch: 87 Loss: 2.7359e+07\n",
      "Epoch: 88 Loss: 2.7300e+07\n",
      "Epoch: 89 Loss: 2.7244e+07\n",
      "Epoch: 90 Loss: 2.7191e+07\n",
      "Epoch: 91 Loss: 2.7140e+07\n",
      "Epoch: 92 Loss: 2.7092e+07\n",
      "Epoch: 93 Loss: 2.7045e+07\n",
      "Epoch: 94 Loss: 2.7001e+07\n",
      "Epoch: 95 Loss: 2.6959e+07\n",
      "Epoch: 96 Loss: 2.6919e+07\n",
      "Epoch: 97 Loss: 2.6880e+07\n",
      "Epoch: 98 Loss: 2.6844e+07\n",
      "Epoch: 99 Loss: 2.6808e+07\n",
      "El error final fue: 2.6808e+07\n",
      "Mejores hiperparámetros: (0.01, 3, None)\n",
      "Pérdida de validación con los mejores hiperparámetros: 40019052.17837612\n"
     ]
    }
   ],
   "source": [
    "# Función para evaluar diferentes hiperparámetros en el conjunto de validación\n",
    "def evaluate_hyperparameters(train_df, val_df, learning_rates, degrees, regularizations):\n",
    "    best_val_loss = float('inf')\n",
    "    best_hyperparameters = None\n",
    "    \n",
    "    X_train = train_df.drop(columns=['charges']).values\n",
    "    Y_train = train_df[['charges']].values\n",
    "    X_val = val_df.drop(columns=['charges']).values\n",
    "    Y_val = val_df[['charges']].values\n",
    "\n",
    "    X_train, _, _ = standardize(X_train)\n",
    "    X_val, _, _ = standardize(X_val)\n",
    "\n",
    "    for lr in learning_rates:\n",
    "        for degree in degrees:\n",
    "            for reg in regularizations:\n",
    "                model = linear_regressor(degree=degree)\n",
    "                model.fit_model(X_train, Y_train, lr=lr, epochs=100, l2=reg)\n",
    "                predictions = model.predict(X_val)\n",
    "                val_loss = np.mean((predictions - Y_val)**2)\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_hyperparameters = (lr, degree, reg)\n",
    "    \n",
    "    return best_hyperparameters, best_val_loss\n",
    "\n",
    "# Especificar rangos para hiperparámetros\n",
    "learning_rates = [0.01, 0.001]\n",
    "degrees = [1, 2, 3]\n",
    "regularizations = [None, 0.1, 0.01]\n",
    "\n",
    "best_hyperparameters, best_val_loss = evaluate_hyperparameters(train_df, val_df, learning_rates, degrees, regularizations)\n",
    "\n",
    "print(\"Mejores hiperparámetros:\", best_hyperparameters)\n",
    "print(\"Pérdida de validación con los mejores hiperparámetros:\", best_val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punto 2 Regresion logistica: Convertimos los datos de la variable ob en binaria del fumador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_df['smoker'] = patients_df['smoker'].map({'yes': 1, 'no': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividimos el conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = patients_df.sample(frac=0.7, random_state=200)\n",
    "rest_df = patients_df.drop(train_df.index)\n",
    "val_df = rest_df.sample(frac=0.5, random_state=200)\n",
    "test_df = rest_df.drop(val_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementamos regresion logistica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(data, mean=None, std=None):\n",
    "    if mean is None or std is None:\n",
    "        mean = np.mean(data, axis=0)\n",
    "        std = np.std(data, axis=0)\n",
    "    standardized_data = (data - mean) / std\n",
    "    return standardized_data, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressor():\n",
    "    def __init__(self, degree=1, l2=None):\n",
    "        self.theta = None\n",
    "        self.degree = degree\n",
    "        self.l2 = l2\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit_model(self, X, Y, lr=0.01, epochs=100):\n",
    "        X_poly = polynomial_features(X, self.degree)\n",
    "        n, m = X_poly.shape\n",
    "        self.theta = np.random.rand(m + 1, 1)\n",
    "        X_c = np.hstack((np.ones((n, 1)), X_poly))\n",
    "        for epoch in range(epochs):\n",
    "            predictions = self.sigmoid(X_c.dot(self.theta))\n",
    "            error = predictions - Y\n",
    "            regularization = (self.l2 / n) * self.theta if self.l2 else 0\n",
    "            self.theta -= lr * (X_c.T.dot(error) + regularization)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_poly = polynomial_features(X, self.degree)\n",
    "        X_c = np.hstack((np.ones((X_poly.shape[0], 1)), X_poly))\n",
    "        predictions = self.sigmoid(X_c.dot(self.theta))\n",
    "        # Importante pa que el codigo no se vaya a inf o none\n",
    "        return np.clip(predictions, 0.0001, 0.9999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluamos los hiperparametros basicamente igual que con regresion lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr=0.01, degree=1, reg=None, val_loss=nan\n",
      "lr=0.01, degree=1, reg=0.1, val_loss=nan\n",
      "lr=0.01, degree=1, reg=0.01, val_loss=nan\n",
      "lr=0.01, degree=2, reg=None, val_loss=nan\n",
      "lr=0.01, degree=2, reg=0.1, val_loss=nan\n",
      "lr=0.01, degree=2, reg=0.01, val_loss=nan\n",
      "lr=0.01, degree=3, reg=None, val_loss=nan\n",
      "lr=0.01, degree=3, reg=0.1, val_loss=nan\n",
      "lr=0.01, degree=3, reg=0.01, val_loss=nan\n",
      "lr=0.001, degree=1, reg=None, val_loss=nan\n",
      "lr=0.001, degree=1, reg=0.1, val_loss=nan\n",
      "lr=0.001, degree=1, reg=0.01, val_loss=nan\n",
      "lr=0.001, degree=2, reg=None, val_loss=nan\n",
      "lr=0.001, degree=2, reg=0.1, val_loss=nan\n",
      "lr=0.001, degree=2, reg=0.01, val_loss=nan\n",
      "lr=0.001, degree=3, reg=None, val_loss=nan\n",
      "lr=0.001, degree=3, reg=0.1, val_loss=nan\n",
      "lr=0.001, degree=3, reg=0.01, val_loss=nan\n",
      "Mejores hiperparámetros: None\n",
      "Pérdida de validación con los mejores hiperparámetros: inf\n"
     ]
    }
   ],
   "source": [
    "def evaluate_hyperparameters(train_df, val_df, learning_rates, degrees, regularizations):\n",
    "    best_val_loss = float('inf')\n",
    "    best_hyperparameters = None\n",
    "    \n",
    "    X_train = train_df.drop(columns=['smoker', 'charges']).values\n",
    "    Y_train = train_df[['smoker']].values\n",
    "    X_val = val_df.drop(columns=['smoker', 'charges']).values\n",
    "    Y_val = val_df[['smoker']].values\n",
    "\n",
    "    X_train, mean_train, std_train = standardize(X_train)\n",
    "    X_val, _, _ = standardize(X_val, mean_train, std_train)\n",
    "    \n",
    "    for lr in learning_rates:\n",
    "        for degree in degrees:\n",
    "            for reg in regularizations:\n",
    "                model = LogisticRegressor(degree=degree, l2=reg)\n",
    "                model.fit_model(X_train, Y_train, lr=lr)\n",
    "                predictions = model.predict(X_val)\n",
    "                val_loss = -np.mean(Y_val * np.log(predictions) + (1 - Y_val) * np.log(1 - predictions))\n",
    "                print(f\"lr={lr}, degree={degree}, reg={reg}, val_loss={val_loss}\")  # Mensaje de depuración\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_hyperparameters = (lr, degree, reg)\n",
    "    return best_hyperparameters, best_val_loss\n",
    "    learning_rates = [0.01, 0.001]\n",
    "degrees = [1, 2, 3]\n",
    "regularizations = [None, 0.1, 0.01]\n",
    "\n",
    "best_hyperparameters, best_val_loss = evaluate_hyperparameters(train_df, val_df, learning_rates, degrees, regularizations)\n",
    "\n",
    "print(\"Mejores hiperparámetros:\", best_hyperparameters)\n",
    "print(\"Pérdida de validación con los mejores hiperparámetros:\", best_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr=0.01, degree=1, reg=None, val_loss=nan\n",
      "lr=0.01, degree=1, reg=0.1, val_loss=nan\n",
      "lr=0.01, degree=1, reg=0.01, val_loss=nan\n",
      "lr=0.01, degree=2, reg=None, val_loss=nan\n",
      "lr=0.01, degree=2, reg=0.1, val_loss=nan\n",
      "lr=0.01, degree=2, reg=0.01, val_loss=nan\n",
      "lr=0.01, degree=3, reg=None, val_loss=nan\n",
      "lr=0.01, degree=3, reg=0.1, val_loss=nan\n",
      "lr=0.01, degree=3, reg=0.01, val_loss=nan\n",
      "lr=0.001, degree=1, reg=None, val_loss=nan\n",
      "lr=0.001, degree=1, reg=0.1, val_loss=nan\n",
      "lr=0.001, degree=1, reg=0.01, val_loss=nan\n",
      "lr=0.001, degree=2, reg=None, val_loss=nan\n",
      "lr=0.001, degree=2, reg=0.1, val_loss=nan\n",
      "lr=0.001, degree=2, reg=0.01, val_loss=nan\n",
      "lr=0.001, degree=3, reg=None, val_loss=nan\n",
      "lr=0.001, degree=3, reg=0.1, val_loss=nan\n",
      "lr=0.001, degree=3, reg=0.01, val_loss=nan\n",
      "Mejores hiperparámetros: None\n",
      "Pérdida de validación con los mejores hiperparámetros: inf\n"
     ]
    }
   ],
   "source": [
    "class LogisticRegressor():\n",
    "    def __init__(self, degree=1, l2=None):\n",
    "        self.theta = None\n",
    "        self.degree = degree\n",
    "        self.l2 = l2\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit_model(self, X, Y, lr=0.01, epochs=100):\n",
    "        X_poly = polynomial_features(X, self.degree)\n",
    "        n, m = X_poly.shape\n",
    "        self.theta = np.zeros((m + 1, 1))  # Inicializar theta con ceros\n",
    "        X_c = np.hstack((np.ones((n, 1)), X_poly))\n",
    "        for epoch in range(epochs):\n",
    "            predictions = self.sigmoid(X_c.dot(self.theta))\n",
    "            error = predictions - Y\n",
    "            regularization = (self.l2 / n) * self.theta if self.l2 else 0\n",
    "            self.theta -= lr * (X_c.T.dot(error) + regularization)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_poly = polynomial_features(X, self.degree)\n",
    "        X_c = np.hstack((np.ones((X_poly.shape[0], 1)), X_poly))\n",
    "        predictions = self.sigmoid(X_c.dot(self.theta))\n",
    "        return np.clip(predictions, 0.0001, 0.9999)\n",
    "\n",
    "# mensajes de depuración\n",
    "def evaluate_hyperparameters(train_df, val_df, learning_rates, degrees, regularizations):\n",
    "    best_val_loss = float('inf')\n",
    "    best_hyperparameters = None\n",
    "    \n",
    "    X_train = train_df.drop(columns=['smoker', 'charges']).values\n",
    "    Y_train = train_df[['smoker']].values\n",
    "    X_val = val_df.drop(columns=['smoker', 'charges']).values\n",
    "    Y_val = val_df[['smoker']].values\n",
    "\n",
    "    X_train, mean_train, std_train = standardize(X_train)\n",
    "    X_val, _, _ = standardize(X_val, mean_train, std_train)\n",
    "    \n",
    "    for lr in learning_rates:\n",
    "        for degree in degrees:\n",
    "            for reg in regularizations:\n",
    "                model = LogisticRegressor(degree=degree, l2=reg)\n",
    "                model.fit_model(X_train, Y_train, lr=lr)\n",
    "                predictions = model.predict(X_val)\n",
    "                val_loss = -np.mean(Y_val * np.log(predictions) + (1 - Y_val) * np.log(1 - predictions))\n",
    "                print(f\"lr={lr}, degree={degree}, reg={reg}, val_loss={val_loss}\")\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_hyperparameters = (lr, degree, reg)\n",
    "    \n",
    "    return best_hyperparameters, best_val_loss\n",
    "learning_rates = [0.01, 0.001]\n",
    "degrees = [1, 2, 3]\n",
    "regularizations = [None, 0.1, 0.01]\n",
    "\n",
    "best_hyperparameters, best_val_loss = evaluate_hyperparameters(train_df, val_df, learning_rates, degrees, regularizations)\n",
    "\n",
    "print(\"Mejores hiperparámetros:\", best_hyperparameters)\n",
    "print(\"Pérdida de validación con los mejores hiperparámetros:\", best_val_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
